# streamlit/test_ui2.py
"""
FlowNote í†µí•© UI - íŒŒì¼ ì—…ë¡œë“œ â†’ LangChain ë¶„ë¥˜ â†’ ë©”íƒ€ë°ì´í„° í‘œì‹œ
"""
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ì¤‘ìš”!!!)
project_root = Path(__file__).parent.parent 
sys.path.insert(0, str(project_root))

# ë‘ ë²ˆì§¸: Streamlit + í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
import streamlit as st
from dotenv import load_dotenv

# ë¡œì»¬ì—ì„œëŠ” .env ë¡œë“œ
load_dotenv()

# ì„¸ ë²ˆì§¸: ë°°í¬ í™˜ê²½ì—ì„œëŠ” Streamlit Secrets ë¡œë“œ
try:
    if hasattr(st, 'secrets') and len(st.secrets) > 0:
        for key in ["EMBEDDING_API_KEY", "EMBEDDING_BASE_URL", "EMBEDDING_MODEL",
                    "EMBEDDING_LARGE_API_KEY", "EMBEDDING_LARGE_BASE_URL", "EMBEDDING_LARGE_MODEL",
                    "GPT4O_API_KEY", "GPT4O_BASE_URL", "GPT4O_MODEL",
                    "GPT4O_MINI_API_KEY", "GPT4O_MINI_BASE_URL", "GPT4O_MINI_MODEL",
                    "GPT41_API_KEY", "GPT41_BASE_URL", "GPT41_MODEL"]:
            if key in st.secrets:
                os.environ[key] = st.secrets[key]
except:
    pass


from datetime import datetime
import json
import numpy as np

# ë„¤ ë²ˆì§¸: ë‹¤ìŒ ì„í¬íŠ¸
from backend.classifier.para_agent_wrapper import run_para_agent_sync
from backend.database.metadata_schema import ClassificationMetadataExtender
from backend.database.connection import DatabaseConnection
from backend.utils import load_pdf
from backend.embedding import EmbeddingGenerator
from backend.chunking import TextChunker
from backend.faiss_search import FAISSRetriever
from backend.metadata import FileMetadata
from backend.search_history import SearchHistory
from backend.export import MarkdownExporter

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="FlowNote",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS - ì• í”Œ ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* ì „ì²´ ë°°ê²½ */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1 {
        color: #1d1d1f;
        font-weight: 600;
        letter-spacing: -0.5px;
    }
    
    /* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .upload-card, .search-card, .result-card {
        background: white;
        border-radius: 16px;
        padding: 24px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.07);
        margin: 16px 0;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        border-radius: 12px;
        font-weight: 500;
        border: none;
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 12px rgba(0, 0, 0, 0.1);
    }
    
    /* íƒ­ ìŠ¤íƒ€ì¼ */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    
    .stTabs [data-baseweb="tab"] {
        border-radius: 12px 12px 0 0;
        padding: 12px 24px;
        font-weight: 500;
    }
    
    /* ë©”íŠ¸ë¦­ ì¹´ë“œ */
    [data-testid="stMetricValue"] {
        font-size: 28px;
        font-weight: 600;
    }
    
    /* ì…ë ¥ í•„ë“œ */
    .stTextInput>div>div>input {
        border-radius: 12px;
        border: 2px solid #e5e5e7;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "classification_history" not in st.session_state:
    st.session_state.classification_history = []

if "db_extender" not in st.session_state:
    st.session_state.db_extender = ClassificationMetadataExtender()

if "faiss_retriever" not in st.session_state:
    st.session_state.faiss_retriever = None

if "file_metadata" not in st.session_state:
    st.session_state.file_metadata = FileMetadata()

if "search_history" not in st.session_state:
    st.session_state.search_history = SearchHistory()

if "last_search_results" not in st.session_state:
    st.session_state.last_search_results = None

if "last_search_query" not in st.session_state:
    st.session_state.last_search_query = None

# íƒ€ì´í‹€
st.title("ğŸ“š FlowNote")
st.markdown("**AI ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ & ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ**")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    
    # íŒŒì¼ í†µê³„
    if st.session_state.file_metadata.metadata:
        stats = st.session_state.file_metadata.get_statistics()
        st.metric("ğŸ“ ì „ì²´ íŒŒì¼", stats['total_files'])
        st.metric("ğŸ“¦ ì „ì²´ ì²­í¬", stats['total_chunks'])
        st.metric("ğŸ’¾ ì´ í¬ê¸°", f"{stats['total_size_mb']} MB")
    
    st.divider()
    
    # ë¶„ë¥˜ íˆìŠ¤í† ë¦¬
    st.subheader("ğŸ¤– ë¶„ë¥˜ íˆìŠ¤í† ë¦¬")
    if st.session_state.classification_history:
        for item in st.session_state.classification_history[-5:]:
            with st.expander(f"ğŸ“„ {item['filename'][:20]}..."):
                st.write(f"**ì¹´í…Œê³ ë¦¬**: {item['category']}")
                st.write(f"**ì‹ ë¢°ë„**: {item['confidence']:.0%}")
                st.caption(item['timestamp'])
    else:
        st.info("ì•„ì§ ë¶„ë¥˜ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    st.divider()
    
    # ê²€ìƒ‰ íˆìŠ¤í† ë¦¬
    st.subheader("ğŸ” ìµœê·¼ ê²€ìƒ‰")
    history = st.session_state.search_history.get_recent_searches(3)
    if history:
        for item in history:
            st.caption(f"ğŸ” {item['query']}")
            st.caption(f"   {item['results_count']}ê°œ ê²°ê³¼")
    else:
        st.info("ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤")

# ë©”ì¸ íƒ­
tab1, tab2, tab3 = st.tabs([
    "ğŸ  í™ˆ - ê²€ìƒ‰ & ì—…ë¡œë“œ",
    "ğŸ¤– ìë™ ë¶„ë¥˜",
    "ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ"
])

# ============================================================================
# TAB 1: í™ˆ - í†µí•© ê²€ìƒ‰ & ì—…ë¡œë“œ
# ============================================================================
with tab1:
    col1, col2 = st.columns([1, 1])
    
    # ì™¼ìª½: íŒŒì¼ ì—…ë¡œë“œ
    with col1:
        st.markdown("### ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        st.markdown('<div class="upload-card">', unsafe_allow_html=True)
        
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, TXT, MD)",
            type=['pdf', 'txt', 'md'],
            accept_multiple_files=True,
            help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        )
        
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì„ íƒë¨")
            
            if st.button("ğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘", type="primary", width='stretch'):
                doc_list = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # íŒŒì¼ ì½ê¸°
                for i, uploaded_file in enumerate(uploaded_files):
                    status_text.text(f"ğŸ“„ {uploaded_file.name} ì²˜ë¦¬ ì¤‘...")
                    try:
                        if uploaded_file.type == "application/pdf":
                            content = load_pdf(uploaded_file)
                        else:
                            content = uploaded_file.read().decode('utf-8')
                        
                        doc_list.append({
                            'name': uploaded_file.name,
                            'content': content,
                            'size': uploaded_file.size,
                            'type': uploaded_file.type
                        })
                    except Exception as e:
                        st.error(f"âŒ {uploaded_file.name}: {str(e)}")
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                if doc_list:
                    status_text.text("ğŸ”® ì„ë² ë”© ìƒì„± ì¤‘...")
                    
                    # ì²­í‚¹
                    chunker = TextChunker()
                    all_chunks = []
                    chunk_metadata = []
                    
                    for doc in doc_list:
                        chunks = chunker.chunk_text(doc['content'])
                        all_chunks.extend(chunks)
                        for chunk in chunks:
                            chunk_metadata.append({
                                'filename': doc['name'],
                                'file_type': doc['type']
                            })
                    
                    # ì„ë² ë”©
                    embedder = EmbeddingGenerator()
                    result = embedder.generate_embeddings(all_chunks)
                    embeddings_array = np.array(result['embeddings'])
                    
                    # FAISS ì¸ë±ìŠ¤
                    documents = []
                    for chunk, meta in zip(all_chunks, chunk_metadata):
                        documents.append({
                            "content": chunk,
                            "metadata": meta
                        })
                    
                    retriever = FAISSRetriever(dimension=embeddings_array.shape[1])
                    retriever.add_documents(embeddings_array, documents)
                    st.session_state.faiss_retriever = retriever
                    
                    # ë©”íƒ€ë°ì´í„° ì €ì¥
                    for doc in doc_list:
                        count = sum(1 for m in chunk_metadata if m['filename'] == doc['name'])
                        st.session_state.file_metadata.add_file(
                            file_name=doc['name'],
                            file_size=doc['size'],
                            chunk_count=count,
                            embedding_dim=embeddings_array.shape[1]
                        )
                    
                    status_text.empty()
                    progress_bar.empty()
                    st.success(f"âœ… {len(doc_list)}ê°œ íŒŒì¼, {len(all_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ì˜¤ë¥¸ìª½: ê²€ìƒ‰
    with col2:
        st.markdown("### ğŸ” ë¬¸ì„œ ê²€ìƒ‰")
        st.markdown('<div class="search-card">', unsafe_allow_html=True)
        
        if st.session_state.faiss_retriever:
            query = st.text_input(
                "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: FlowNote ì‚¬ìš©ë²•",
                help="ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
            )
            
            col_k, col_btn = st.columns([1, 2])
            with col_k:
                k = st.slider("ê²°ê³¼ ê°œìˆ˜", 1, 10, 3)
            
            with col_btn:
                search_button = st.button("ğŸ” ê²€ìƒ‰", type="primary", width='stretch')
            
            if query and search_button:
                with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                    results = st.session_state.faiss_retriever.search(query, k=k)
                    st.session_state.search_history.add_search(
                        query=query,
                        results_count=len(results)
                    )
                    st.session_state.last_search_results = results
                    st.session_state.last_search_query = query
                
                st.success(f"âœ… {len(results)}ê°œ ê²°ê³¼ ë°œê²¬!")
        else:
            st.info("ğŸ’¡ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if st.session_state.last_search_results:
        st.divider()
        st.markdown("### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼")
        
        for i, result in enumerate(st.session_state.last_search_results, 1):
            with st.expander(
                f"**ê²°ê³¼ #{i}** | {result['metadata']['filename']} | ìœ ì‚¬ë„: {result['score']:.2%}",
                expanded=(i == 1)
            ):
                st.markdown(result['content'])
                st.caption(f"íŒŒì¼: {result['metadata']['filename']}")
        
        # ê²°ê³¼ ì €ì¥
        col_export, col_clear = st.columns([3, 1])
        with col_export:
            if st.button("ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ MDë¡œ ì €ì¥", width='stretch'):
                exporter = MarkdownExporter()
                md_content = exporter.export_search_results(
                    query=st.session_state.last_search_query,
                    results=st.session_state.last_search_results
                )
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"flownote_search_{timestamp}.md"
                st.download_button(
                    label="â¬‡ï¸ ë‹¤ìš´ë¡œë“œ",
                    data=md_content,
                    file_name=filename,
                    mime="text/markdown",
                    width='stretch'
                )

# ============================================================================
# TAB 2: ìë™ ë¶„ë¥˜
# ============================================================================
with tab2:
    st.header("ğŸ¤– AI ìë™ ë¶„ë¥˜")
    
    uploaded_file = st.file_uploader(
        "ë¶„ë¥˜í•  íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf', 'txt', 'md'],
        help="PDF, TXT, MD íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤"
    )
    
    if uploaded_file:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ“„ íŒŒì¼ëª…", uploaded_file.name)
        with col2:
            st.metric("ğŸ“¦ í¬ê¸°", f"{uploaded_file.size / 1024:.2f} KB")
        with col3:
            st.metric("ğŸ·ï¸ íƒ€ì…", uploaded_file.type.split('/')[-1].upper())
        
        # ë¶„ë¥˜ ë²„íŠ¼
        if st.button("ğŸš€ ë¶„ë¥˜ ì‹œì‘", type="primary", width='stretch'):
            with st.spinner("ğŸ¤– AIê°€ íŒŒì¼ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    # 1. íŒŒì¼ ì½ê¸°
                    if uploaded_file.type == "application/pdf":
                        text = load_pdf(uploaded_file)
                    else:
                        text = uploaded_file.read().decode('utf-8')
                    
                    # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    metadata = {
                        "filename": uploaded_file.name,
                        "file_size": uploaded_file.size,
                        "file_type": uploaded_file.type,
                        "uploaded_at": datetime.now().isoformat()
                    }
                    
                    # 3. LangChain + LangGraph ê¸°ë°˜ ë¶„ë¥˜
                    classification_result = run_para_agent_sync(
                        text=text[:2000],
                        metadata=metadata
                    )
                    
                    # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                    file_id = st.session_state.db_extender.save_classification_result(
                        result=classification_result,
                        filename=uploaded_file.name
                    )
                    
                    # 5. íˆìŠ¤í† ë¦¬ ì €ì¥
                    history_item = {
                        "filename": uploaded_file.name,
                        "category": classification_result.get('category', 'Unknown'),
                        "confidence": classification_result.get('confidence', 0),
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "file_id": file_id
                    }
                    st.session_state.classification_history.append(history_item)
                    
                    st.success("âœ… ë¶„ë¥˜ ì™„ë£Œ!")
                    
                    # 6. ê²°ê³¼ í‘œì‹œ
                    st.markdown("---")
                    st.subheader("ğŸ“Š ë¶„ë¥˜ ê²°ê³¼")
                    
                    # ì¹´í…Œê³ ë¦¬ ì•„ì´ì½˜
                    category_icons = {
                        "Projects": "ğŸš€",
                        "Areas": "ğŸ¯",
                        "Resources": "ğŸ“š",
                        "Archives": "ğŸ“¦"
                    }
                    
                    category = classification_result.get('category', 'Unknown')
                    icon = category_icons.get(category, "â“")
                    
                    # ê²°ê³¼ ì¹´ë“œ
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"### {icon} {category}")
                        st.progress(classification_result.get('confidence', 0))
                        st.caption(f"ì‹ ë¢°ë„: {classification_result.get('confidence', 0):.0%}")
                    
                    with col2:
                        if classification_result.get('conflict_detected', False):
                            st.warning("âš ï¸ ì¶©ëŒ ê°ì§€ë¨")
                        else:
                            st.success("âœ… ëª…í™•í•œ ë¶„ë¥˜")
                        
                        if classification_result.get('requires_review', False):
                            st.info("ğŸ‘€ ê²€í†  í•„ìš”")
                    
                    # ìƒì„¸ ì •ë³´
                    with st.expander("ğŸ“ ë¶„ë¥˜ ê·¼ê±°", expanded=True):
                        st.markdown(classification_result.get('reasoning', 'ì •ë³´ ì—†ìŒ'))
                    
                    with st.expander("ğŸ”‘ í‚¤ì›Œë“œ íƒœê·¸"):
                        tags = classification_result.get('keyword_tags', [])
                        if tags:
                            st.write(", ".join([f"`{tag}`" for tag in tags[:10]]))
                        else:
                            st.caption("í‚¤ì›Œë“œ ì—†ìŒ")
                    
                    # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                    with st.expander("ğŸ“‹ ë©”íƒ€ë°ì´í„°"):
                        st.json({
                            "filename": uploaded_file.name,
                            "file_size_kb": f"{uploaded_file.size / 1024:.2f}",
                            "file_type": uploaded_file.type,
                            "classified_at": datetime.now().isoformat(),
                            "file_id": file_id
                        })
                    
                except Exception as e:
                    st.error(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")

# ============================================================================
# TAB 3: í†µê³„ ëŒ€ì‹œë³´ë“œ
# ============================================================================
with tab3:
    st.header("ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ")
    
    # ìƒë‹¨ KPI
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_files = len(st.session_state.classification_history)
        st.metric("ğŸ“ ë¶„ë¥˜ëœ íŒŒì¼", total_files)
    
    with col2:
        if st.session_state.faiss_retriever:
            st.metric("ğŸ” ì¸ë±ìŠ¤ í¬ê¸°", st.session_state.faiss_retriever.size())
        else:
            st.metric("ğŸ” ì¸ë±ìŠ¤ í¬ê¸°", 0)
    
    with col3:
        search_count = len(st.session_state.search_history.get_all_searches())
        st.metric("ğŸ” ì´ ê²€ìƒ‰", search_count)
    
    with col4:
        if st.session_state.classification_history:
            avg_conf = sum(item['confidence'] for item in st.session_state.classification_history) / len(st.session_state.classification_history)
            st.metric("â­ í‰ê·  ì‹ ë¢°ë„", f"{avg_conf:.0%}")
        else:
            st.metric("â­ í‰ê·  ì‹ ë¢°ë„", "0%")
    
    st.divider()
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    if st.session_state.classification_history:
        from collections import Counter
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬")
            categories = [item['category'] for item in st.session_state.classification_history]
            category_counts = Counter(categories)
            
            for category, count in category_counts.most_common():
                icon = {"Projects": "ğŸš€", "Areas": "ğŸ¯", "Resources": "ğŸ“š", "Archives": "ğŸ“¦"}.get(category, "â“")
                st.metric(f"{icon} {category}", count)
        
        with col2:
            st.subheader("ğŸ“ˆ ìµœê·¼ í™œë™")
            for item in st.session_state.classification_history[-5:]:
                with st.container():
                    st.markdown(f"**{item['filename']}**")
                    st.caption(f"{item['category']} | {item['timestamp']}")
                    st.divider()
    else:
        st.info("ğŸ“Š ì•„ì§ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. TAB 2ì—ì„œ íŒŒì¼ì„ ë¶„ë¥˜í•´ë³´ì„¸ìš”!")

# í•˜ë‹¨ ì •ë³´
st.divider()
st.caption("FlowNote v3.2 | LangChain + LangGraph í†µí•© | Made with â¤ï¸ by Jay")
