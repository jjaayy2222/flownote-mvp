# streamlit/test_ui5.py

"""
FlowNote í†µí•© UI - ì˜¨ë³´ë”© í”Œë¡œìš° ì¶”ê°€
- main
    - tab1 : ì˜¨ë³´ë”©
    - tab2 : íŒŒì¼ ì—…ë¡œë“œ & ë¶„ë¥˜
    - tab3 : í‚¤ì›Œë“œ ê²€ìƒ‰
    - tab4 : íŒŒì¼ í†µê³„ (â† tab2ì˜ ì •ë³´ ì‹¤ì‹œê°„ ë°˜ì˜ë˜ë„ë¡ ìˆ˜ì •)
    - tab5 : ë©”íƒ€ë°ì´í„° + ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ í•„í„°ë§ ì¶”ê°€
- ì‚¬ì´ë“œë°”
    - ì˜¨ë³´ë”© ìƒíƒœ ì¶”ê°€
    - ë¶„ë¥˜ íˆìŠ¤í† ë¦¬
"""

import requests 
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent 
sys.path.insert(0, str(project_root))

# Streamlit + í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
import streamlit as st
from dotenv import load_dotenv

# ë¡œì»¬ì—ì„œëŠ” .env ë¡œë“œ
load_dotenv()

# ë°°í¬ í™˜ê²½ì—ì„œëŠ” Streamlit Secrets ë¡œë“œ
try:
    if hasattr(st, 'secrets') and len(st.secrets) > 0:
        for key in ["EMBEDDING_API_KEY", "EMBEDDING_BASE_URL", "EMBEDDING_MODEL",
                    "EMBEDDING_LARGE_API_KEY", "EMBEDDING_LARGE_BASE_URL", "EMBEDDING_LARGE_MODEL",
                    "GPT4O_API_KEY", "GPT4O_BASE_URL", "GPT4O_MODEL",
                    "GPT4O_MINI_API_KEY", "GPT4O_MINI_BASE_URL", "GPT4O_MINI_MODEL",
                    "GPT41_API_KEY", "GPT41_BASE_URL", "GPT41_MODEL"]:
            if key in st.secrets:
                os.environ[key] = st.secrets[key]
except:
    pass

from datetime import datetime
import json
import pandas as pd
import numpy as np

# Backend ì„í¬íŠ¸
from backend.embedding import EmbeddingGenerator
from backend.chunking import TextChunker
from backend.faiss_search import FAISSRetriever
from backend.metadata import FileMetadata
from backend.search_history import SearchHistory
from backend.classifier.para_classifier import PARAClassifier
from backend.validators import FileValidator
from backend.exceptions import FileValidationError
from backend.classifier.para_agent_wrapper import run_para_agent_sync
from backend.database.metadata_schema import ClassificationMetadataExtender
from backend.database.connection import DatabaseConnection
from backend.utils import format_file_size, load_pdf
from backend.export import MarkdownExporter
from backend.modules import extract_text_from_pdf

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="FlowNote í†µí•© UI í…ŒìŠ¤íŠ¸",
    page_icon="ğŸ“š",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "classification_history" not in st.session_state:
    st.session_state.classification_history = []

if "db_extender" not in st.session_state:
    st.session_state.db_extender = ClassificationMetadataExtender()

# ì˜¨ë³´ë”© í”Œë¡œìš°ìš© ì„¸ì…˜ ìƒíƒœ
if "onboarding_step" not in st.session_state:
    st.session_state.onboarding_step = 1

if "onboarding_user_id" not in st.session_state:
    st.session_state.onboarding_user_id = None

if "onboarding_name" not in st.session_state:
    st.session_state.onboarding_name = ""

if "onboarding_occupation" not in st.session_state:
    st.session_state.onboarding_occupation = ""

if "suggested_areas" not in st.session_state:
    st.session_state.suggested_areas = []

if "selected_areas" not in st.session_state:
    st.session_state.selected_areas = []


# íŒŒì¼ ì €ì¥ ì •ì˜ í•¨ìˆ˜
def save_to_para_folder(filename, content, category):
    base_path = Path("data/exports")
    category_path = base_path / category
    category_path.mkdir(parents=True, exist_ok=True)
    file_path = category_path / filename
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    return str(file_path)


# ==========================
# íƒ€ì´í‹€
# ==========================

st.title("ğŸ“š FlowNote í†µí•© í…ŒìŠ¤íŠ¸ UI")
st.markdown("**ì˜¨ë³´ë”© â†’ ë¶„ë¥˜ â†’ í‚¤ì›Œë“œ ê²€ìƒ‰ â†’ í†µê³„ â†’ ë©”íƒ€ë°ì´í„°**")


# ==========================
# ì‚¬ì´ë“œë°”: ë¶„ë¥˜ íˆìŠ¤í† ë¦¬ ë“±
# ==========================
with st.sidebar:
    st.header("ğŸ‘¤ ì‚¬ìš©ì ì •ë³´")
    
    if st.session_state.onboarding_step == 3:
        st.success("âœ… ì˜¨ë³´ë”© ì™„ë£Œ")
        st.write(f"ì´ë¦„: {st.session_state.onboarding_name}")
        st.write(f"ì§ì—…: {st.session_state.onboarding_occupation}")
        st.write(f"User ID: {st.session_state.onboarding_user_id[:12]}...")
    else:
        st.warning("âš ï¸ ì˜¨ë³´ë”© í•„ìš”")
        st.info("Tab1ì—ì„œ ì˜¨ë³´ë”©ì„ ì™„ë£Œí•˜ì„¸ìš”")

    st.divider()

    # ë¶„ë¥˜ íˆìŠ¤í† ë¦¬
    st.header("ğŸ“Š ë¶„ë¥˜ íˆìŠ¤í† ë¦¬")
    if st.session_state.classification_history:
        st.metric("ì´ ë¶„ë¥˜ íŒŒì¼", len(st.session_state.classification_history))
        with st.expander("ìµœê·¼ ë¶„ë¥˜ ê²°ê³¼", expanded=True):
            for idx, item in enumerate(reversed(st.session_state.classification_history[-5:]), 1):
                st.markdown(f"**{idx}. {item['filename']}**")
                st.caption(f"ì¹´í…Œê³ ë¦¬: {item['category']} ({item['confidence']:.0%})")
                st.caption(f"ì‹œê°„: {item['timestamp']}")
        if st.button("ì´ˆê¸°í™”", key="clear_history"):
            st.session_state.classification_history = []
            st.rerun()
    else:
        st.info("ì•„ì§ ë¶„ë¥˜ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")


# ==========================
# main (tab1, 2, 3, 4, 5)
# ==========================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ğŸš€ ì˜¨ë³´ë”©",
    "ğŸ“¤ íŒŒì¼ ë¶„ë¥˜",
    "ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰",
    "ğŸ¯ ë¶„ë¥˜ í†µê³„",
    "ğŸ“Š ë©”íƒ€ë°ì´í„°"
])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TAB 1: ì˜¨ë³´ë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    st.header("ğŸš€ ì˜¨ë³´ë”© í”Œë¡œìš°")
    if st.session_state.onboarding_step == 1:
        # Step 1
        with st.form("step1_form"):
            name = st.text_input("ì´ë¦„", value=st.session_state.onboarding_name, placeholder="ì˜ˆ: í™ê¸¸ë™")
            occupation = st.text_input("ì§ì—…", value=st.session_state.onboarding_occupation, placeholder="ì˜ˆ: ê°œë°œì, ë””ìì´ë„ˆ")
            submitted = st.form_submit_button("ë‹¤ìŒ ë‹¨ê³„ â†’")
            if submitted:
                if not name or not occupation:
                    st.error("âš ï¸ ì´ë¦„/ì§ì—…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
                else:
                    # API ìš”ì²­
                    response1 = requests.post(
                        "http://127.0.0.1:8000/api/onboarding/step1",
                        json={"occupation": occupation, "name": name})
                    if response1.status_code == 200:
                        result1 = response1.json()
                        user_id = result1['user_id']
                        response2 = requests.get(
                            f"http://127.0.0.1:8000/api/onboarding/suggest-areas?user_id={user_id}&occupation={occupation}")
                        if response2.status_code == 200:
                            result2 = response2.json()
                            st.session_state.onboarding_user_id = user_id
                            st.session_state.onboarding_name = name
                            st.session_state.onboarding_occupation = occupation
                            # ì˜ì—­ ì²˜ë¦¬
                            if 'suggested_areas' in result2:
                                st.session_state.suggested_areas = result2['suggested_areas']
                            elif 'areas' in result2:
                                st.session_state.suggested_areas = result2['areas']
                            st.session_state.onboarding_step = 2
                            st.rerun()
                        else:
                            st.error(f"ì˜ì—­ ì¶”ì²œ ì‹¤íŒ¨: {response2.status_code}")
                    else:
                        st.error(f"Step1 ì‹¤íŒ¨: {response1.status_code}")
    elif st.session_state.onboarding_step == 2:
        # Step 2
        st.subheader("Step 2: ê´€ì‹¬ ì˜ì—­ ì„ íƒ")
        for area in st.session_state.suggested_areas:
            checked = st.checkbox(area, key=f"area_{area}")
            if checked and area not in st.session_state.selected_areas:
                st.session_state.selected_areas.append(area)
            elif not checked and area in st.session_state.selected_areas:
                st.session_state.selected_areas.remove(area)
        col1, col2 = st.columns(2)
        with col1:
            if st.button("â† ì´ì „", key="onboarding_prev"):
                st.session_state.onboarding_step = 1
                st.rerun()
        with col2:
            if st.button("ì €ì¥í•˜ê³  ì‹œì‘ â†’", key="onboarding_next", disabled=(len(st.session_state.selected_areas) == 0)):
                response = requests.post(
                    "http://127.0.0.1:8000/api/onboarding/save-context",
                    json={"user_id": st.session_state.onboarding_user_id, "selected_areas": st.session_state.selected_areas}
                )
                if response.status_code == 200:
                    st.session_state.onboarding_step = 3
                    st.rerun()
                else:
                    st.error(f"ì €ì¥ ì‹¤íŒ¨: {response.status_code}")
    elif st.session_state.onboarding_step == 3:
        st.subheader("ğŸ‰ ì˜¨ë³´ë”© ì™„ë£Œ!")
        st.success(f"{st.session_state.onboarding_name}ë‹˜ì˜ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.write(f"ì§ì—…: {st.session_state.onboarding_occupation}")
        st.write(f"ì‚¬ìš©ì ID: {st.session_state.onboarding_user_id}")
        st.write("ì„ íƒ ì˜ì—­:")
        for area in st.session_state.selected_areas:
            st.write(f"- {area}")
        if st.button("ì˜¨ë³´ë”© ë‹¤ì‹œí•˜ê¸°"):
            st.session_state.onboarding_step = 1
            st.session_state.onboarding_user_id = None
            st.session_state.onboarding_name = ""
            st.session_state.onboarding_occupation = ""
            st.session_state.suggested_areas = []
            st.session_state.selected_areas = []
            st.rerun()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TAB 2: íŒŒì¼ ë¶„ë¥˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with tab2:
    st.header("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ & ìë™ ë¶„ë¥˜")
    
    # âœ… ì˜¨ë³´ë”© ì™„ë£Œ ì—¬ë¶€ í™•ì¸
    onboarding_complete = (
        st.session_state.onboarding_step == 3 and 
        st.session_state.onboarding_user_id is not None
    )
    
    if not onboarding_complete:
        st.warning("âš ï¸ ë¨¼ì € ì˜¨ë³´ë”©ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”! (Tab1)")
        st.info("ì˜¨ë³´ë”©ì„ ì™„ë£Œí•˜ë©´ ë‹¹ì‹ ì˜ ë§¥ë½ì— ë§ëŠ” ì •í™•í•œ ë¶„ë¥˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
        st.stop()
    
    # âœ… ì˜¨ë³´ë”© ì •ë³´ í‘œì‹œ
    with st.expander("ğŸ‘¤ í˜„ì¬ ì‚¬ìš©ì ì •ë³´", expanded=False):
        st.write(f"**ì´ë¦„:** {st.session_state.onboarding_name}")
        st.write(f"**ì§ì—…:** {st.session_state.onboarding_occupation}")
        st.write(f"**User ID:** {st.session_state.onboarding_user_id}")
        st.write(f"**ê´€ì‹¬ ì˜ì—­:**")
        for area in st.session_state.selected_areas:
            st.write(f"  - {area}")
    
    uploaded_file = st.file_uploader(
        "ë¶„ë¥˜í•  íŒŒì¼ ì—…ë¡œë“œ", type=['pdf', 'txt', 'md'], key="file_uploader_tab2"
    )
    
    if uploaded_file:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("íŒŒì¼ëª…", uploaded_file.name)
        with col2:
            st.metric("íŒŒì¼ í¬ê¸°", f"{uploaded_file.size / 1024:.2f} KB")
        with col3:
            st.metric("íŒŒì¼ íƒ€ì…", uploaded_file.type.split('/')[-1].upper())
        
        # ë¶„ë¥˜ ë²„íŠ¼
        if st.button("ğŸš€ ë¶„ë¥˜ ì‹œì‘", key="classify_btn_tab2"):
            with st.spinner("AI ë¶„ì„ ì¤‘... (ì‚¬ìš©ì ë§¥ë½ ë°˜ì˜)"):
                try:
                    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if uploaded_file.type == "application/pdf":
                        text = load_pdf(uploaded_file)
                    else:
                        text = uploaded_file.read().decode('utf-8')
                    
                    # 2. ë©”íƒ€ë°ì´í„° êµ¬ì„± (âœ¨ user_id ì¶”ê°€!)
                    metadata = {
                        "filename": uploaded_file.name,
                        "file_size": uploaded_file.size,
                        "file_type": uploaded_file.type,
                        "uploaded_at": datetime.now().isoformat(),
                        # ì‚¬ìš©ì ì •ë³´ ì¶”ê°€
                        "user_id": st.session_state.onboarding_user_id,
                        "user_name": st.session_state.onboarding_name,
                        "user_occupation": st.session_state.onboarding_occupation,
                        "user_areas": st.session_state.selected_areas
                    }
                    
                    # 3. ë¶„ë¥˜ ì‹¤í–‰ (âœ¨ user_context ì¶”ê°€!)
                    from backend.classifier.context_injector import get_context_injector
                    
                    injector = get_context_injector()
                    
                    # 3-1. ê¸°ë³¸ ë¶„ë¥˜
                    classification_result = run_para_agent_sync(
                        text=text[:2000],
                        metadata=metadata
                    )
                    
                    # 3-2. ì‚¬ìš©ì ë§¥ë½ ì£¼ì…
                    classification_result = injector.inject_context_from_user_id(
                        user_id=st.session_state.onboarding_user_id,
                        ai_result=classification_result
                    )
                    
                    # 4. DB ì €ì¥
                    file_id = st.session_state.db_extender.save_classification_result(
                        result=classification_result,
                        filename=uploaded_file.name
                    )
                    
                    # 5. íˆìŠ¤í† ë¦¬ ì €ì¥
                    history_item = {
                        "filename": uploaded_file.name,
                        "category": classification_result.get('category', 'Unknown'),
                        "confidence": classification_result.get('confidence', 0),
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "file_id": file_id,
                        "user_id": st.session_state.onboarding_user_id,
                        "context_injected": classification_result.get('context_injected', False)
                    }
                    st.session_state.classification_history.append(history_item)
                    
                    # 6. ê²°ê³¼ í‘œì‹œ
                    st.success("âœ… ë¶„ë¥˜ ì™„ë£Œ!")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ì¹´í…Œê³ ë¦¬", classification_result.get('category', 'N/A'))
                        st.metric("ì‹ ë¢°ë„", f"{classification_result.get('confidence', 0):.0%}")
                    
                    with col2:
                        st.metric("ë§¥ë½ ë°˜ì˜", 
                                "âœ… ë°˜ì˜ë¨" if classification_result.get('context_injected') else "âŒ ë¯¸ë°˜ì˜")
                        keyword_tags = classification_result.get('keyword_tags', [])
                        st.metric("í‚¤ì›Œë“œ ìˆ˜", len(keyword_tags))
                    
                    # 7. ìƒì„¸ ì •ë³´
                    with st.expander("ğŸ“Š ìƒì„¸ ë¶„ë¥˜ ì •ë³´", expanded=True):
                        st.json(classification_result)
                    
                except Exception as e:
                    st.error(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
                    st.exception(e)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TAB 3: í‚¤ì›Œë“œ ê²€ìƒ‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with tab3:
    st.header("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰")
    
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ (PDF, TXT, MD)",
        type=['pdf', 'txt', 'md'],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸ“„ íŒŒì¼ ì²˜ë¦¬"):
        doc_list = []
        
        with st.status("íŒŒì¼ ì²˜ë¦¬ ì¤‘...", expanded=True) as status:
            for uploaded_file in uploaded_files:
                try:
                    if uploaded_file.type == "application/pdf":
                        content = load_pdf(uploaded_file)
                    else:
                        content = uploaded_file.read().decode('utf-8')
                    
                    doc_list.append({
                        'name': uploaded_file.name,
                        'content': content,
                        'size': uploaded_file.size,
                        'type': uploaded_file.type
                    })
                    
                except Exception as e:
                    st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            if doc_list:
                st.write("ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘...")
                chunker = TextChunker()
                all_chunks = []
                chunk_metadata = []
                
                for doc in doc_list:
                    chunks = chunker.chunk_text(doc['content'])
                    all_chunks.extend(chunks)
                    for chunk in chunks:
                        chunk_metadata.append({
                            'filename': doc['name'],
                            'file_type': doc['type'],
                            # í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë„ ë„£ê¸°
                        })
                
                st.write("ğŸ”® ì„ë² ë”© ìƒì„± ì¤‘...")
                embedder = EmbeddingGenerator()
                result = embedder.generate_embeddings(all_chunks)
                
                embeddings_list = result['embeddings']
                embeddings_array = np.array(embeddings_list)
                
                st.write("ğŸ” ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
                                
                retriever = FAISSRetriever(dimension=embeddings_array.shape[1])
                retriever.add_documents(embeddings_array, [
                    {"content": chunk, "metadata": meta}
                    for chunk, meta in zip(all_chunks, chunk_metadata)
                ])
                
                st.session_state.faiss_retriever = retriever
                
                st.success(f"âœ… {len(doc_list)}ê°œ íŒŒì¼, {len(all_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
                
    retriever_exists = st.session_state.get('faiss_retriever') is not None
    if retriever_exists:
        st.divider()
        query = st.text_input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 10, 3)
        search_clicked = st.button("ê²€ìƒ‰")
        if query and search_clicked:
            try:
                results = st.session_state['faiss_retriever'].search(query, k=k)
            except Exception as e:
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                results = []
            
            if 'search_history' not in st.session_state:
                st.session_state['search_history'] = []
            st.session_state['search_history'].append({
                "query": query,
                "results_count": len(results),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            st.session_state['last_search_results'] = results
            st.session_state['last_search_query'] = query
            
            st.subheader(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ)")
            
            for i, result in enumerate(results, 1):
                meta = result.get('metadata', {})
                filename = meta.get('filename', 'unknown')
                filetype = meta.get('file_type', 'unknown')
                score = result.get('score', 0.0)
                keywords = meta.get('keyword_tags', [])
                confidence = meta.get('confidence_score', None)
                conf_text = f"{confidence:.0%}" if confidence is not None else "-"
                keywords_text = ", ".join(keywords[:5]) if keywords else "-"
                
                with st.expander(f"ê²°ê³¼ #{i} | {filename} | {filetype} | ì ìˆ˜: {score:.4f}"):
                    st.markdown(result.get('content', ''))
                    st.markdown(f"**í‚¤ì›Œë“œ:** {keywords_text}")
                    st.markdown(f"**ì‹ ë¢°ë„:** {conf_text}")
        
        last_results = st.session_state.get('last_search_results')
        last_query = st.session_state.get('last_search_query', '')
        if last_results:
            st.divider()
            export_clicked = st.button("ğŸ“¥ ê²€ìƒ‰ ê²°ê³¼ MDë¡œ ë‚´ë³´ë‚´ê¸°", width='stretch')
            if export_clicked:
                try:
                    exporter = MarkdownExporter()
                    md_content = exporter.export_search_results(query=last_query, results=last_results)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"flownote_search_{timestamp}.md"
                    st.download_button(
                        label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                        data=md_content,
                        file_name=filename,
                        mime="text/markdown",
                        width='stretch'
                    )
                except Exception as e:
                    st.error(f"MD ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
    else:
        st.info("ğŸ“¤ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TAB 4: ë¶„ë¥˜ í†µê³„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with tab4:
    st.header("ğŸ¯ ë¶„ë¥˜ í†µê³„")
    if st.session_state.classification_history:
        from collections import Counter
        categories = [item['category'] for item in st.session_state.classification_history]
        category_counts = Counter(categories)
        st.metric("Projects", category_counts.get('Projects', 0))
        st.metric("Areas", category_counts.get('Areas', 0))
        st.metric("Resources", category_counts.get('Resources', 0))
        st.metric("Archives", category_counts.get('Archives', 0))
        confidences = [item['confidence'] for item in st.session_state.classification_history]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        st.metric("í‰ê·  ì‹ ë¢°ë„", f"{avg_confidence:.0%}")
        st.bar_chart(category_counts)
    else:
        st.info("ë¶„ë¥˜ íŒŒì¼ ì—†ìŒ")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# TAB 5: ë©”íƒ€ë°ì´í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with tab5:
    st.header("ğŸ“Š ë©”íƒ€ë°ì´í„° í™•ì¸")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        st.subheader("í˜„ì¬ ì„¸ì…˜ ë¶„ë¥˜ ê²°ê³¼")
    with col2:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", key="refresh_metadata"):
            st.rerun()
        # ì‚¬ìš©ì ID í•„í„°
        user_filter = st.selectbox(
            "ğŸ” ì‚¬ìš©ì í•„í„°",
            options=["ì „ì²´"] + list(set([
                item.get('user_id', 'N/A')[:12] 
                for item in st.session_state.classification_history
            ])),
            key="user_filter"
        )
    
    # 1. í˜„ì¬ ì„¸ì…˜ ë°ì´í„° (st.session_state.classification_history)
    if st.session_state.classification_history:
        st.markdown("### ğŸ“ ì´ë²ˆ ì„¸ì…˜ ë¶„ë¥˜ ëª©ë¡")
        
        # í•„í„°ë§ ë¡œì§ ì ìš©
        filtered_history = st.session_state.classification_history
        
        if user_filter != "ì „ì²´":
            filtered_history = [
                item for item in st.session_state.classification_history
                if item.get('user_id', '').startswith(user_filter)
            ]
        
        session_data = []
        for item in st.session_state.classification_history:
            session_data.append({
                "íŒŒì¼ëª…": item['filename'],
                "ì¹´í…Œê³ ë¦¬": item['category'],
                "ì‹ ë¢°ë„": f"{item['confidence']:.0%}",
                "ì‹œê°„": item['timestamp'],
                "ë§¥ë½": "âœ…" if item.get('context_injected', False) else "âŒ",
                "User ID": item.get('user_id', 'N/A')[:12] + "..."
            })
        
        df_session = pd.DataFrame(session_data)
        st.dataframe(df_session, width='stretch')
        
        # í•„í„°ë§ëœ í†µê³„
        st.divider()
        col1, col2, col3, col4, col5= st.columns(5)
        
        with col1:
            st.metric("í•„í„° ê²°ê³¼", len(filtered_history))
        
        with col2:
            st.metric("ì´ íŒŒì¼", len(st.session_state.classification_history))
        
        with col3:
            if filtered_history:
                avg_conf = sum(item['confidence'] for item in filtered_history) / len(filtered_history)
                st.metric("í‰ê·  ì‹ ë¢°ë„", f"{avg_conf:.0%}")
        
        with col4:
            context_count = sum(1 for item in filtered_history if item.get('context_injected', False))
            st.metric("ë§¥ë½ ë°˜ì˜", f"{context_count}/{len(filtered_history)}")
        
        with col5:
            if st.button("ğŸ—‘ï¸ ì„¸ì…˜ ì´ˆê¸°í™”"):
                st.session_state.classification_history = []
                st.rerun()
    
    else:
        st.info("í˜„ì¬ ì„¸ì…˜ì—ì„œ ë¶„ë¥˜ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2. DBì— ì €ì¥ëœ ì „ì²´ ë°ì´í„° (ì„ íƒì‚¬í•­)
    st.divider()
    with st.expander("ğŸ—„ï¸ ì „ì²´ DB ë©”íƒ€ë°ì´í„° ë³´ê¸°"):
        try:
            all_classifications = st.session_state.db_extender.get_all_classifications()
            
            if all_classifications:
                df_data = []
                for item in all_classifications:
                    df_data.append({
                        "íŒŒì¼ëª…": item['filename'],
                        "ì¹´í…Œê³ ë¦¬": item['para_category'],
                        "ì‹ ë¢°ë„": f"{item['confidence_score']:.0%}",
                        "í‚¤ì›Œë“œ": item['keyword_tags'][:50] if item['keyword_tags'] else "",
                        "ì¶©ëŒ": "âš ï¸" if item['conflict_flag'] else "âœ…",
                        "Snapshot ID": item['snapshot_id'][:20] if item['snapshot_id'] else ""
                    })
                
                df_all = pd.DataFrame(df_data)
                st.dataframe(df_all, width='stretch')
                st.caption(f"ì´ {len(all_classifications)}ê°œ í•­ëª©")
            else:
                st.info("DBì— ì €ì¥ëœ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        except Exception as e:
            st.error(f"DB ë¡œë“œ ì‹¤íŒ¨: {e}")


# í•˜ë‹¨ ì •ë³´
st.divider()
st.caption("FlowNote MVP v3.4 | ì‚¬ìš©ì ë§¥ë½ í†µí•© ì¤‘ | Made with â¤ï¸ by Jay")
