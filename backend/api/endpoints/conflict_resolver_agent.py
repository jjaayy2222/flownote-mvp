# backend/api/endpoints/conflict_resolver_agent.py

"""
Conflict Resolution Agent (LangGraph ê¸°ë°˜)
- para_agent.py êµ¬ì¡° ì™„ë²½ ì¬í™œìš©
- ì¶©ëŒ ê°ì§€ â†’ ë¶„ì„ â†’ í•´ê²°ì±… ì œì•ˆ â†’ ì„ íƒ â†’ ì ìš©
"""

from typing import TypedDict, List, Dict, Any
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
import json
import logging

from backend.config import ModelConfig
from backend.api.models import (
    ConflictRecord,
    ConflictReport,
    ConflictType,
    ResolutionMethod,
    ResolutionStatus,
    ConflictResolution,
    ResolutionStrategy
)

logger = logging.getLogger(__name__)

# ============================================
# State ì •ì˜
# ============================================

class ConflictResolutionState(TypedDict):
    """Conflict Resolverì˜ ìƒíƒœ"""
    conflicts: List[ConflictRecord]              # ì…ë ¥: ê°ì§€ëœ ì¶©ëŒë“¤
    current_conflict: ConflictRecord             # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì¶©ëŒ
    analysis_result: Dict[str, Any]              # ë¶„ì„ ê²°ê³¼
    suggested_strategies: List[ResolutionStrategy]  # ì œì•ˆëœ í•´ê²°ì±…ë“¤
    selected_strategy: ResolutionStrategy        # ì„ íƒëœ ìµœì¢… í•´ê²°ì±…
    resolutions: List[ConflictResolution]        # ëª¨ë“  í•´ê²° ê²°ê³¼
    final_report: ConflictReport                 # ìµœì¢… ë³´ê³ ì„œ


# ============================================
# ì´ìŠ¤ì¼€ì´í”„ í•¨ìˆ˜ 
# ============================================

def _escape_prompt_braces(content: str) -> str:
    """
    í”„ë¡¬í”„íŠ¸ì˜ ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„
    {conflict_info} ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ëª¨ë“  { } ë¥¼ {{ }} ë¡œ ë³€í™˜
    """
    lines = []
    for line in content.split('\n'):
        if '{conflict_info}' in line:
            lines.append(line)
        else:
            escaped_line = line.replace('{', '{{').replace('}', '}}')
            escaped_line = escaped_line.replace('{{{{', '{{').replace('}}}}', '}}')
            lines.append(escaped_line)
    return '\n'.join(lines)


# ============================================
# Prompt ë¡œë“œ í•¨ìˆ˜
# ============================================

def load_conflict_resolution_prompt() -> str:
    """ì¶©ëŒ í•´ê²° í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
    prompt_path = Path(__file__).parent.parent.parent / "classifier" / "prompts" / "conflict_resolution_prompt.txt"
    
    try:
        with open(prompt_path, "r", encoding="utf-8") as f:
            template_content = f.read()  # â† ë³€ìˆ˜ì— ì €ì¥
        
        # âœ… ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬!!!
        escaped_content = _escape_prompt_braces(template_content)
        return escaped_content
        
    except FileNotFoundError:
        logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì—†ìŒ: {prompt_path}")
        # Fallback í”„ë¡¬í”„íŠ¸
        return "í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨"


# ============================================
# Node 1: Analyze (ì¶©ëŒ ë¶„ì„)
# ============================================

def analyze_conflict_node(state: ConflictResolutionState) -> ConflictResolutionState:
    """
    ì¶©ëŒ ë¶„ì„ ë…¸ë“œ
    - LLMì„ ì‚¬ìš©í•´ ì¶©ëŒì˜ ì‹¬ê°ë„, ì›ì¸, ì˜í–¥ ë¶„ì„
    """
    conflict = state["current_conflict"]
    
    logger.info(f"ğŸ” ì¶©ëŒ ë¶„ì„ ì‹œì‘: {conflict.type}")
    
    # LLMìœ¼ë¡œ ë¶„ì„
    llm = ChatOpenAI(
        api_key=ModelConfig.GPT4O_MINI_API_KEY,
        base_url=ModelConfig.GPT4O_MINI_BASE_URL,
        model=ModelConfig.GPT4O_MINI_MODEL,
        temperature=0.3
    )
    
    analysis_prompt = f"""
ì¶©ëŒ ë¶„ì„:

ìœ í˜•: {conflict.type}
ì„¤ëª…: {conflict.description}
ì‹¬ê°ë„: {conflict.severity}
ìë™ í•´ê²° ê°€ëŠ¥: {conflict.auto_resolvable}

ì´ ì¶©ëŒì˜ ì›ì¸ê³¼ ì˜í–¥ì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
{{
  "root_cause": "ì›ì¸ ì„¤ëª…",
  "impact": "ì˜í–¥ ë¶„ì„",
  "priority": "high|medium|low",
  "recommended_approach": "ìë™|ìˆ˜ë™"
}}
    """.strip()
    
    try:
        response = llm.invoke(analysis_prompt)
        analysis_text = response.content
        
        # ë¹ˆ ì‘ë‹µ ë°©ì§€!!!
        if not analysis_text or not analysis_text.strip():
            logger.warning(f"âš ï¸ ë¹ˆ ë¶„ì„ ê²°ê³¼, Fallback ì‚¬ìš©")
            raise ValueError("Empty response from LLM")        
        
        # JSON ì¶”ì¶œ
        analysis_result = json.loads(analysis_text)
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {analysis_result.get('priority')}")
        
        return {
            **state,
            "analysis_result": analysis_result
        }
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        # Fallback
        return {
            **state,
            "analysis_result": {
                "root_cause": "ë¶„ì„ ì‹¤íŒ¨",
                "impact": "ì•Œ ìˆ˜ ì—†ìŒ",
                "priority": "medium",
                "recommended_approach": "ìˆ˜ë™"
            }
        }


# ============================================
# Node 2: Suggest (í•´ê²°ì±… ì œì•ˆ)
# ============================================

def suggest_strategies_node(state: ConflictResolutionState) -> ConflictResolutionState:
    """
    í•´ê²°ì±… ì œì•ˆ ë…¸ë“œ
    - ì¶©ëŒ ìœ í˜•ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3-5ê°œì˜ í•´ê²° ì „ëµ ì œì•ˆ
    """
    conflict = state["current_conflict"]
    analysis = state["analysis_result"]
    
    logger.info(f"ğŸ’¡ í•´ê²°ì±… ì œì•ˆ ì‹œì‘")
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_template = load_conflict_resolution_prompt()
    
    llm = ChatOpenAI(
        api_key=ModelConfig.GPT4O_MINI_API_KEY,
        base_url=ModelConfig.GPT4O_MINI_BASE_URL,
        model=ModelConfig.GPT4O_MINI_MODEL,
        temperature=0.5
    )
    
    conflict_info = f"""
ìœ í˜•: {conflict.type}
ì„¤ëª…: {conflict.description}
ì‹¬ê°ë„: {conflict.severity}
ë¶„ì„ ê²°ê³¼: {json.dumps(analysis, ensure_ascii=False)}
    """.strip()
    
    try:
        response = llm.invoke(prompt_template.format(conflict_info=conflict_info))
        strategies_text = response.content
        
        # JSON íŒŒì‹±
        strategies_json = json.loads(strategies_text)
        
        # ResolutionStrategy ê°ì²´ë¡œ ë³€í™˜
        strategies = []
        for s in strategies_json:
            strategy = ResolutionStrategy(
                conflict_id=conflict.conflict_id,
                method=ResolutionMethod(s["method"]),
                recommended_value=s["recommended_value"],
                confidence=s["confidence"],
                reasoning=s["reasoning"],
                affected_files=s.get("affected_files", [])
            )
            strategies.append(strategy)
        
        logger.info(f"âœ… {len(strategies)}ê°œ ì „ëµ ì œì•ˆ ì™„ë£Œ")
        
        return {
            **state,
            "suggested_strategies": strategies
        }
        
    except Exception as e:
        logger.error(f"âŒ ì „ëµ ì œì•ˆ ì‹¤íŒ¨: {e}")
        
        # Fallback ì „ëµ - ì•ˆì „í•œ ì˜µì…˜ë§Œ!!!
        fallback_strategy = ResolutionStrategy(
            conflict_id=conflict.conflict_id,
            method=ResolutionMethod.AUTO_BY_CONFIDENCE,  # â† ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë©”ì„œë“œ!
            recommended_value="ìë™ í•´ê²° (ê¸°ë³¸ê°’)",
            confidence=0.5,
            reasoning="ìë™ í•´ê²° ì‹¤íŒ¨, ê¸°ë³¸ ì „ëµ ì‚¬ìš©",
            affected_files=[]
        )
        
        return {
            **state,
            "suggested_strategies": [fallback_strategy]
        }



# ============================================
# Node 3: Select (ìµœì  ì „ëµ ì„ íƒ)
# ============================================

def select_best_strategy_node(state: ConflictResolutionState) -> ConflictResolutionState:
    """
    ìµœì  ì „ëµ ì„ íƒ ë…¸ë“œ
    - ì‹ ë¢°ë„, ìë™ í•´ê²° ê°€ëŠ¥ ì—¬ë¶€ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœê³ ì˜ ì „ëµ ì„ íƒ
    """
    strategies = state["suggested_strategies"]
    conflict = state["current_conflict"]
    
    logger.info(f"ğŸ¯ ìµœì  ì „ëµ ì„ íƒ ì‹œì‘")
    
    # ì „ëµ ì ìˆ˜ ê³„ì‚°
    def calculate_score(strategy: ResolutionStrategy) -> float:
        score = strategy.confidence  # ê¸°ë³¸ ì ìˆ˜
        
        # ìë™ í•´ê²° ê°€ëŠ¥í•˜ë©´ ê°€ì‚°ì 
        if conflict.auto_resolvable and strategy.method != ResolutionMethod.MANUAL_OVERRIDE:
            score += 0.1
        
        # ì˜í–¥ë°›ëŠ” íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ ê°€ì‚°ì 
        if len(strategy.affected_files) <= 3:
            score += 0.05
        
        return min(score, 1.0)
    
    # ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    scored_strategies = [(s, calculate_score(s)) for s in strategies]
    scored_strategies.sort(key=lambda x: x[1], reverse=True)
    
    # ìµœê³  ì ìˆ˜ ì „ëµ ì„ íƒ
    best_strategy, best_score = scored_strategies[0]
    
    logger.info(f"âœ… ì„ íƒëœ ì „ëµ: {best_strategy.method} (ì ìˆ˜: {best_score:.2f})")
    
    return {
        **state,
        "selected_strategy": best_strategy
    }


# ============================================
# Node 4: Apply (í•´ê²°ì±… ì ìš©)
# ============================================

def apply_resolution_node(state: ConflictResolutionState) -> ConflictResolutionState:
    """
    í•´ê²°ì±… ì ìš© ë…¸ë“œ
    - ì„ íƒëœ ì „ëµì„ ì‹¤ì œë¡œ ì ìš©í•˜ê³  ConflictResolution ìƒì„±
    """
    strategy = state["selected_strategy"]
    conflict = state["current_conflict"]
    
    logger.info(f"âš™ï¸ í•´ê²°ì±… ì ìš© ì‹œì‘")
    
    # ConflictResolution ìƒì„±
    resolution = ConflictResolution(
        conflict_id=conflict.conflict_id,
        status=ResolutionStatus.RESOLVED if conflict.auto_resolvable else ResolutionStatus.PENDING_REVIEW,
        strategy=strategy,
        resolved_by="system" if conflict.auto_resolvable else "pending_user",
        notes=f"ìë™ í•´ê²°: {strategy.method}" if conflict.auto_resolvable else "ìˆ˜ë™ ê²€í†  í•„ìš”"
    )
    
    # í•´ê²° ê²°ê³¼ ì¶”ê°€
    resolutions = state.get("resolutions", [])
    resolutions.append(resolution)
    
    logger.info(f"âœ… í•´ê²° ì™„ë£Œ: {resolution.status}")
    
    return {
        **state,
        "resolutions": resolutions
    }


# ============================================
# Node 5: Generate Report (ìµœì¢… ë³´ê³ ì„œ ìƒì„±)
# ============================================

def generate_report_node(state: ConflictResolutionState) -> ConflictResolutionState:
    """
    ìµœì¢… ë³´ê³ ì„œ ìƒì„± ë…¸ë“œ
    """
    conflicts = state["conflicts"]
    resolutions = state["resolutions"]
    
    logger.info(f"ğŸ“Š ìµœì¢… ë³´ê³ ì„œ ìƒì„±")
    
    # í†µê³„ ê³„ì‚°
    total_conflicts = len(conflicts)
    auto_resolved = sum(1 for r in resolutions if r.status == ResolutionStatus.RESOLVED)
    manual_review = sum(1 for r in resolutions if r.status == ResolutionStatus.PENDING_REVIEW)
    
    # ì¶©ëŒ ìœ í˜•ë³„ ë¶„ë¥˜
    conflict_breakdown = {}
    for c in conflicts:
        conflict_breakdown[c.type] = conflict_breakdown.get(c.type, 0) + 1
    
    # ConflictReport ìƒì„±
    report = ConflictReport(
        total_conflicts=total_conflicts,
        detected_conflicts=conflicts,
        resolutions=resolutions,
        conflict_breakdown=conflict_breakdown,
        auto_resolved_count=auto_resolved,
        manual_review_needed=manual_review,
        resolution_rate=auto_resolved / total_conflicts if total_conflicts > 0 else 0.0,
        status="completed",
        summary=f"{total_conflicts}ê°œ ì¶©ëŒ ì¤‘ {auto_resolved}ê°œ ìë™ í•´ê²°, {manual_review}ê°œ ìˆ˜ë™ ê²€í†  í•„ìš”"
    )
    
    logger.info(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    
    return {
        **state,
        "final_report": report
    }


# ============================================
# Graph êµ¬ì„±
# ============================================

def create_conflict_resolver_graph():
    """Conflict Resolver Graph ìƒì„±"""
    graph = StateGraph(ConflictResolutionState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("analyze", analyze_conflict_node)
    graph.add_node("suggest", suggest_strategies_node)
    graph.add_node("select", select_best_strategy_node)
    graph.add_node("apply", apply_resolution_node)
    graph.add_node("generate_report", generate_report_node)
    
    # ì—£ì§€ ì¶”ê°€
    graph.add_edge(START, "analyze")
    graph.add_edge("analyze", "suggest")
    graph.add_edge("suggest", "select")
    graph.add_edge("select", "apply")
    graph.add_edge("apply", "generate_report")
    graph.add_edge("generate_report", END)
    
    return graph.compile()


# ============================================
# ë©”ì¸ í•¨ìˆ˜ (ë™ê¸° ë²„ì „)
# ============================================

def resolve_conflicts_sync(conflicts: List[ConflictRecord]) -> ConflictReport:
    """
    ì¶©ëŒ í•´ê²° (ë™ê¸° ë²„ì „)
    
    Args:
        conflicts: í•´ê²°í•  ì¶©ëŒ ëª©ë¡
        
    Returns:
        ConflictReport: ìµœì¢… ë³´ê³ ì„œ
    """
    logger.info(f"ğŸš€ ì¶©ëŒ í•´ê²° ì‹œì‘: {len(conflicts)}ê°œ")
    
    graph = create_conflict_resolver_graph()
    
    # ê° ì¶©ëŒì„ ìˆœì°¨ ì²˜ë¦¬
    all_resolutions = []
    
    for conflict in conflicts:
        initial_state = {
            "conflicts": conflicts,
            "current_conflict": conflict,
            "analysis_result": {},
            "suggested_strategies": [],
            "selected_strategy": None,
            "resolutions": all_resolutions,
            "final_report": None
        }
        
        # Graph ì‹¤í–‰
        result = graph.invoke(initial_state)
        all_resolutions = result["resolutions"]
    
    # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    final_state = {
        "conflicts": conflicts,
        "resolutions": all_resolutions,
        "current_conflict": None,
        "analysis_result": {},
        "suggested_strategies": [],
        "selected_strategy": None,
        "final_report": None
    }
    
    final_result = generate_report_node(final_state)
    
    logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ")
    
    return final_result["final_report"]


# ============================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ============================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ì¶©ëŒ ìƒì„±
    test_conflicts = [
        ConflictRecord(
            type=ConflictType.KEYWORD_CONFLICT,
            description="ìœ ì‚¬ í‚¤ì›Œë“œ: 'python' vs 'py'",
            severity=0.8,
            auto_resolvable=True
        ),
        ConflictRecord(
            type=ConflictType.CATEGORY_CONFLICT,
            description="íŒŒì¼ file_001ì´ ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ì†í•¨",
            severity=0.7,
            auto_resolvable=False
        )
    ]
    
    # í•´ê²° ì‹¤í–‰
    report = resolve_conflicts_sync(test_conflicts)
    
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ë³´ê³ ì„œ")
    print("="*60)
    print(f"ì´ ì¶©ëŒ: {report.total_conflicts}")
    print(f"ìë™ í•´ê²°: {report.auto_resolved_count}")
    print(f"ìˆ˜ë™ ê²€í† : {report.manual_review_needed}")
    print(f"í•´ê²°ë¥ : {report.resolution_rate:.1%}")
    print(f"\nìš”ì•½: {report.summary}")
    print("="*60)




"""test_result_1 - ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ 

    ```bash
    python -c "
    from backend.api.endpoints.conflict_resolver import ConflictDetector
    from backend.api.endpoints.conflict_resolver_agent import resolve_conflicts_sync

    detector = ConflictDetector(data_source='mock')
    report_detect = detector.detect_all()
    report_resolve = resolve_conflicts_sync(report_detect.detected_conflicts)
    print(f'âœ… í•´ê²°ë¥ : {report_resolve.resolution_rate:.1%}')
    "
    âŒ ë¶„ì„ ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)
    âŒ ë¶„ì„ ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)
    âœ… í•´ê²°ë¥ : 100.0%
    ```

"""


