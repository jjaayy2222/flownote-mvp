# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# backend/routes/classifier_routes.py
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
ë¶„ë¥˜ ë¼ìš°íŠ¸
- LangChain ê¸°ë°˜ ë¶„ë¥˜
- ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜s
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
"""
import os
from pathlib import Path
import json
import time

from fastapi import APIRouter, HTTPException, UploadFile, File
from pydantic import BaseModel
from typing import Dict, Optional, List, Any
from datetime import datetime

# í•¨ìˆ˜ ì„í¬íŠ¸
from backend.classifier.langchain_integration import (
    classify_with_langchain,
    classify_with_metadata,
    hybrid_classify
)
from backend.classifier.context_injector import get_context_injector
from backend.classifier.para_agent import run_para_agent


# í´ë˜ìŠ¤ ì„í¬íŠ¸
from backend.classifier.langchain_integration import PARAClassificationOutput
from backend.services.parallel_processor import ParallelClassifier
from backend.classifier.keyword_classifier import KeywordClassifier
from backend.classifier.metadata_classifier import MetadataClassifier
from backend.classifier.para_classifier import PARAClassifier
from backend.classifier.context_injector import ContextInjector
from backend.services.conflict_service import ConflictService
from backend.routes.conflict_routes import ClassifyRequest, ClassifyResponse
from backend.metadata import FileMetadata
from backend.classifier.keyword_classifier import KeywordClassifier
from backend.data_manager import DataManager
from backend.database.metadata_schema import ClassificationMetadataExtender


import logging

logger = logging.getLogger(__name__)


# ============ Router ì´ˆê¸°í™” ============
router = APIRouter()                    # API Router ì¶”ê°€


# ============ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ============
# ìš”ì²­ë§ˆë‹¤ ì¬ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
injector = get_context_injector()



# ============ ìš”ì²­ ìŠ¤í‚¤ë§ˆë“¤ ============

class ClassificationRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ ìš”ì²­"""
    text: str                       # ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
    filename: str = "unknown"       # ì„ íƒì‚¬í•­
    user_id: Optional[str] = None   # ë§¥ë½ ì£¼ì…ìš© / ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ìš©
    

class ClassificationResponse(BaseModel):
    """ë¶„ë¥˜ ì‘ë‹µ"""
    category: str
    confidence: float
    status: str = "success"
    # <--- ë‚˜ë¨¸ì§€ í•„ë“œë“¤

# ìˆ˜ì •: ìƒˆ í”„ë¡¬í”„íŠ¸ì— ë§ì¶˜ ClassifyRequest (user_context í•„ë“œ ì¶”ê°€!)
class ClassifyRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ ìš”ì²­ (ìƒˆ í”„ë¡¬í”„íŠ¸ ë²„ì „)"""
    text: str                               # í•„ìˆ˜: ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
    # ê¸°ì¡´ í•„ë“œë“¤
    user_id: Optional[str] = None
    file_id: Optional[str] = None
    # ìƒˆ í•„ë“œë“¤ (í”„ë¡¬í”„íŠ¸ {occupation}, {areas}, {interests} ëŒ€ì‘)
    occupation: Optional[str] = None        # ì§ì—…
    areas: Optional[List[str]] = []         # ì±…ì„ ì˜ì—­
    interests: Optional[List[str]] = []     # ê´€ì‹¬ì‚¬

class ClassifyResponse(BaseModel):
    """ë¶„ë¥˜ ì‘ë‹µ (ìƒˆ í”„ë¡¬í”„íŠ¸ ë²„ì „)"""
    category: str                           # ìµœì¢… ì¹´í…Œê³ ë¦¬
    confidence: float                       # ì‹ ë¢°ë„
    # ê¸°ì¡´ í•„ë“œë“¤ 
    snapshot_id: Optional[str] = None   
    conflict_detected: bool = False
    requires_review: bool = False
    user_profile: dict = {}
    context_injected: bool = False
    
    # ìƒˆ í•„ë“œë“¤ (keyword_classifier ì¶œë ¥ ë°˜ì˜)
    keyword_tags: List[str]                 # í‚¤ì›Œë“œ íƒœê·¸ (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±)
    reasoning: str                          # ë¶„ë¥˜ ì´ìœ  (í”„ë¡¬í”„íŠ¸ reasoning)
    
    # ì‚¬ìš©ì ë§¥ë½ ê´€ë ¨ (í”„ë¡¬í”„íŠ¸ ë°˜ì˜)
    user_context: Dict[str, Any] = {}       # ì „ë‹¬ëœ user_context 
    user_context_matched: bool = False      # ë§¥ë½ ë§¤ì¹­ ì—¬ë¶€
    user_areas: Optional[List[str]] = []    # ì‚¬ìš©ëœ ì˜ì—­



class MetadataClassifyRequest(BaseModel):
    """ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ìš”ì²­"""
    metadata: Dict
    user_id: Optional[str] = None


class HybridClassifyRequest(BaseModel): 
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ìš”ì²­"""
    text: str
    metadata: Dict
    user_id: Optional[str] = None


class ParallelClassifyRequest(BaseModel):
    """ë³‘ë ¬ ë¶„ë¥˜ ìš”ì²­ (í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„°)"""
    text: str
    metadata: Dict
    filename: str = "unknown"
    user_id: Optional[str] = None


# ============ API ì—”ë“œí¬ì¸íŠ¸ ============

@router.post("/text")
async def classify_text_endpoint(request: ClassificationRequest):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ (LangChain ê¸°ë°˜)"""
    try:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (classify_with_langchain ë“±)
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        user_areas = []
        if request.user_id:
            try:
                user_context = injector.get_user_context(request.user_id)
                user_areas = user_context.get('areas', [])
                
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                logger.info(f"ğŸ“„ ìƒˆ ë¶„ë¥˜ ìš”ì²­")
                logger.info(f"User ID: {request.user_id}")
                logger.info(f"User Areas: {user_areas}")
                logger.info(f"Text Preview: {request.text[:100]}...")
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            except Exception as e:
                logger.warning(f"âš ï¸ Context loading failed: {e}")
        
        # Step 2: AI ë¶„ì„ (LangChain ì‚¬ìš©)
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•¨!
        result = classify_with_langchain(request.text)
        
        # Step 3: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id and user_areas:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
            result["context_injected"] = True
            result["user_areas"] = user_areas
        else:
            result["context_injected"] = False
            result["user_areas"] = []

        # Step 4: ë””ë²„ê¹… ë¡œê·¸
        logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"  - Category: {result.get('category', 'N/A')}")
        logger.info(f"  - Tags: {result.get('tags', [])[:5]}")
        logger.info(f"  - Context Injected: {result.get('context_injected', False)}")
        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return {
            "status": "error",
            "message": str(e)
        }



@router.post("/metadata")
async def classify_metadata_endpoint(request: MetadataClassifyRequest):
    """ë©”íƒ€ë°ì´í„° ë¶„ë¥˜"""
    try:
        result = classify_with_metadata(request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/hybrid")
async def hybrid_classify_endpoint(request: HybridClassifyRequest):
    """í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜"""
    try:
        result = hybrid_classify(request.text, request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/parallel")
async def parallel_classify_endpoint(request: ParallelClassifyRequest):
    """í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° ë³‘ë ¬ ë¶„ë¥˜"""
    try:
        # âš ï¸ ì£¼ì˜: ParallelClassifier.classify_parallelì€ ì •ì  ë©”ì„œë“œ!
        # â†’ ë§¤ë²ˆ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•¨
        result = ParallelClassifier.classify_parallel(
            request.text,
            request.metadata
        )
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ ë³‘ë ¬ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/para")
async def classify_para(request: ClassificationRequest):
    """PARA ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
    
        - /api/classify/para ë¡œ ì ‘ê·¼ ê°€ëŠ¥
    """
    try:
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "category": result.get("category", "Resources"),
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/keywords")
async def classify_keywords(request: ClassificationRequest):
    """í‚¤ì›Œë“œ ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
        - ì ‘ê·¼: POST http://localhost:8000/api/classify/keywords
    """
    try:
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ìš”ì²­: {request.text[:50]}...")
        
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ!
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "category": {
                "keywords": result.get("tags", []),
                "confidence": result.get("confidence", 0.8)
            },
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# /classify ì—”ë“œí¬ì¸íŠ¸
# ============================================================

@router.post("/classify", response_model=ClassifyResponse)
async def classify_text(request: ClassifyRequest):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ API
    
    - ë§¤ë²ˆ ìƒˆë¡œìš´ KeywordClassifier ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    - ë¹„ë™ê¸° aclassify() ì‚¬ìš©
    - ì‚¬ìš©ì ë§¥ë½(occupation, areas, interests) ì™„ì „ ë°˜ì˜
    - ìƒˆ keyword_tags ë§¤ë²ˆ ìƒì„±
    - DB ë° ë¡œê·¸ì— ì €ì¥
    """
    try:
        logger.info(f"ğŸ” ë¶„ë¥˜ ìš”ì²­ ì‹œì‘:")
        logger.info(f"   - Text: {request.text[:50]}...")
        logger.info(f"   - User ID: {request.user_id}")
        logger.info(f"   - File ID: {request.file_id}")
        logger.info(f"   - Occupation: {request.occupation}")
        logger.info(f"   - Areas: {request.areas}")
        logger.info(f"   - Interests: {request.interests}")
        
        # ============================================================
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ============================================================
        
        user_context = {
            "user_id": request.user_id,
            "file_id": request.file_id,
            "occupation": request.occupation or "ì¼ë°˜ ì‚¬ìš©ì",      # ì§ì—…
            "areas": request.areas,                              # ì˜ì—­
            "interests": request.interests,                      # ê´€ì‹¬ì‚¬
            "context_keywords": {                                # ìë™ ìƒì„±
                area: [area, f"{area} ê´€ë ¨", f"{area} ì—…ë¬´", f"{area} í”„ë¡œì íŠ¸"]
                for area in (request.areas or [])
            }
        }
        
        logger.info(f"âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±:")
        logger.info(f"   - Occupation: {user_context['occupation']}")
        logger.info(f"   - Areas: {user_context['areas']}")
        logger.info(f"   - Context Keywords: {list(user_context['context_keywords'].keys())}")

        # ============================================================
        # Step 2: PARA ë¶„ë¥˜ (ë§¤ë²ˆ ìƒˆë¡œ)
        # ============================================================
        try:
            # PARA Agent ì‹¤í–‰
            para_result = await run_para_agent(
                text=request.text,
                metadata={
                    "user_id": request.user_id,
                    "file_id": request.file_id,
                    "occupation": request.occupation,
                    "areas": request.areas,
                    "interests": request.interests          # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
                }
            )
            logger.info(f"âœ… PARA ë¶„ë¥˜ ì™„ë£Œ:")
            logger.info(f"   - Category: {para_result.get('category')}")
            logger.info(f"   - Confidence: {para_result.get('confidence')}")
            logger.info(f"   - Snapshot ID: {para_result.get('snapshot_id')}")
        
        except Exception as para_error:
            logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {para_error}", exc_info=True)
            # ê¸°ë³¸ê°’ ì„¤ì •
            para_result = {
                "category": "Resources",
                "confidence": 0.0,
                "snapshot_id": f"snap_failed_{int(datetime.now().timestamp())}"
            }
        
        # ============================================================
        # Step 3: í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„ë™ê¸° + ìƒˆ ì¸ìŠ¤í„´ìŠ¤)
        # ============================================================
        keyword_classifier = KeywordClassifier()                # ë§¤ë²ˆ ìƒˆ ì¸ìŠ¤í„´ìŠ¤!
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ (Instance ID: {keyword_classifier.instance_id})")
        keyword_result = await keyword_classifier.aclassify(    # ë¹„ë™ê¸° aclassify!
            text=request.text,
            user_context=user_context
        )
        
        # keyword_tags ì¶”ì¶œ ë° ë³´ì¥
        new_keyword_tags = keyword_result.get('tags', ['ê¸°íƒ€'])
        if not isinstance(new_keyword_tags, list):
            new_keyword_tags = [str(new_keyword_tags)] if new_keyword_tags else ['ê¸°íƒ€']
        else:
            new_keyword_tags = [str(tag) for tag in new_keyword_tags if str(tag).strip()]
            if not new_keyword_tags:
                new_keyword_tags = ['ê¸°íƒ€']

        logger.info(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"   - Instance ID: {keyword_result.get('instance_id')}")
        logger.info(f"   - Tags: {new_keyword_tags[:5]}")
        logger.info(f"   - Confidence: {keyword_result.get('confidence')}")
        logger.info(f"   - User Context Matched: {keyword_result.get('user_context_matched')}")
        logger.info(f"   - Processing Time: {keyword_result.get('processing_time')}")

        # ============================================================
        # Step 4: ì¶©ëŒ í•´ê²°
        # ============================================================
        
        conflict_service = ConflictService()
        conflict_result = conflict_service.classify_text(
            para_result=para_result,
            keyword_result=keyword_result,
            text=request.text,
            user_context=user_context                                   # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
        )

        logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ:")
        logger.info(f"   - Final Category: {conflict_result.get('final_category')}")
        logger.info(f"   - Keyword Tags: {conflict_result.get('keyword_tags', new_keyword_tags)}")
        logger.info(f"   - Conflict Detected: {conflict_result.get('conflict_detected')}")
        logger.info(f"   - Requires Review: {conflict_result.get('requires_review')}")


        # ============================================================
        # Step 5: ì™„ì „ ìˆ˜ì •ëœ DataManager + DB ì €ì¥
        # ============================================================

        # 1. DataManager CSV ë¡œê·¸ ëˆ„ì 
        try:
            data_manager = DataManager()
            
            # test_4_classification_log()ì™€ ì •í™•íˆ ë™ì¼í•œ 5ê°œ ë§¤ê°œë³€ìˆ˜ë§Œ ì‚¬ìš©
            csv_log_result = data_manager.log_classification(
                user_id=request.user_id or "anonymous",
                file_name=request.file_id or "unknown",
                ai_prediction=conflict_result.get('final_category') if 'conflict_result' in locals() else 'ê¸°íƒ€',
                user_selected=None,
                confidence=conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0
                # keyword_tags, user_areas ì œê±° - testì™€ ë§¤ê°œë³€ìˆ˜ ì¼ì¹˜ì‹œí‚¤ê¸°
            )
            
            logger.info(f"âœ… CSV ë¡œê·¸ ì €ì¥ ì™„ë£Œ: data/classifications/classification_log.csv")
            csv_saved = True

        except Exception as csv_error:
            logger.warning(f"âš ï¸ CSV ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {csv_error}")
            csv_saved = False

        # 2. ClassificationMetadataExtender DB ì €ì¥ (ì„±ê³µ í™•ì¸ë¨!)
        try:
            from backend.database.metadata_schema import ClassificationMetadataExtender
            
            extender = ClassificationMetadataExtender()
            
            # ì•ˆì „í•œ ë°ì´í„°ë§Œ DBì— ì €ì¥ (Snapshot ê°ì²´ ì œê±°)
            db_result = {
                "category": conflict_result.get('final_category', para_result.get('category', 'Resources')) if 'conflict_result' in locals() else para_result.get('category', 'Resources'),
                "keyword_tags": new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'],
                "confidence": conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0,
                "conflict_detected": conflict_result.get('conflict_detected', False) if 'conflict_result' in locals() else False,
                "requires_review": conflict_result.get('requires_review', False) if 'conflict_result' in locals() else False,
                "snapshot_id": str(para_result.get('snapshot_id', 'snap_unknown')) if 'para_result' in locals() else f"snap_{int(time.time())}",
                "reasoning": conflict_result.get('reason', 'ë¶„ë¥˜ ì™„ë£Œ') if 'conflict_result' in locals() else 'ë¶„ë¥˜ ì™„ë£Œ',
                "user_context": {
                    "user_id": request.user_id or "anonymous",
                    "occupation": request.occupation,
                    "areas": request.areas,
                    "interests": request.interests
                }
            }
            
            db_filename = f"{request.user_id or 'anonymous'}_{int(time.time())}"
            saved_file_id = extender.save_classification_result(
                result=db_result,
                filename=db_filename
            )
            
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: file_id={saved_file_id}")

        except Exception as db_error:
            logger.warning(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {db_error}")
            saved_file_id = None

        # 3. ê°„ë‹¨í•œ JSON ë¡œê·¸ (ëª¨ë“  ê°ì²´ ì•ˆì „ ì²˜ë¦¬)
        try:
            from pathlib import Path
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # ì•ˆì „í•œ JSON ë°ì´í„°ë§Œ ì‚¬ìš©
            safe_snapshot_id = str(para_result.get('snapshot_id', 'snap_unknown')) if 'para_result' in locals() else 'snap_unknown'
            
            simple_log = {
                "timestamp": timestamp,
                "user_id": request.user_id or "anonymous",
                "text_preview": request.text[:100],
                "category": conflict_result.get('final_category', 'Resources') if 'conflict_result' in locals() else 'Resources',
                "confidence": float(conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0),
                "keyword_tags": new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'],
                "snapshot_id": safe_snapshot_id,
                "user_areas": request.areas,
                "matched_context": keyword_result.get('user_context_matched', False) if 'keyword_result' in locals() else False
            }
            
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            LOG_DIR.mkdir(parents=True, exist_ok=True)
            
            json_filename = LOG_DIR / f"classification_clean_{timestamp}.json"
            
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(simple_log, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… JSON ë¡œê·¸ ì €ì¥: {json_filename.name}")
            json_saved = True

        except Exception as json_error:
            logger.warning(f"âš ï¸ JSON ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {json_error}")
            json_saved = False

        # 4. log_info ìƒì„± (ìƒ‰ê¹” ë¬¸ì œ í•´ê²°!)
        log_info = {
            "csv_log": "data/classifications/classification_log.csv",
            "db_saved": saved_file_id is not None,
            "json_log": json_filename.name if 'json_filename' in locals() and json_saved else None,
            "log_directory": "data/log"
        }

        logger.info(f"âœ… Step 5 ì™„ë£Œ - CSV: {csv_saved}, DB: {saved_file_id is not None}, JSON: {json_saved}")


        # ============================================================
        # Step 6: ì‘ë‹µ ë°˜í™˜
        # ============================================================
        
        # ìˆ˜ì • (ìš°ì„ ìˆœìœ„ ì¡°ì •)
        final_category = conflict_result.get('final_category', para_result.get('category', 'ê¸°íƒ€'))
        category = final_category if final_category != 'None' else para_result.get('category', 'Resources')
        
        response = ClassifyResponse(
            category=category,
            confidence=conflict_result.get('confidence', para_result.get('confidence', 0.0)),
            snapshot_id=str(para_result.get('snapshot_id', '')),
            conflict_detected=conflict_result.get('conflict_detected', False),
            requires_review=conflict_result.get('requires_review', False),
            keyword_tags=new_keyword_tags,                      # ìƒˆë¡œ ìƒì„±ëœ í‚¤ì›Œë“œ
            reasoning=conflict_result.get('reason', ''),
            
            # ì‚¬ìš©ì ë§¥ë½ ê´€ë ¨ (ìƒˆ í•„ë“œë“¤)
            user_context_matched=keyword_result.get('user_context_matched', False),
            user_areas=request.areas,                           # ìš”ì²­ëœ ì˜ì—­
            user_context=user_context,                          # ì „ë‹¬ëœ ì „ì²´ ì»¨í…ìŠ¤íŠ¸
            context_injected=len(request.areas) > 0,            # ë§¥ë½ ì£¼ì… ì—¬ë¶€
            log_info=log_info,                                  # ë¡œê·¸ ì •ë³´
            csv_log_result=csv_log_result,                      # CSV ë¡œê·¸ ê²°ê³¼
        )

        logger.info(f"âœ… ì „ì²´ ë¶„ë¥˜ ì™„ë£Œ!")
        logger.info(f"   - Final Category: {response.category}")
        logger.info(f"   - Keyword Tags: {response.keyword_tags[:3]}...")
        logger.info(f"   - User Context Matched: {response.user_context_matched}")
        logger.info(f"   - Total Time: ~{keyword_result.get('processing_time', 'N/A')}")

        return response

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")

# ============================================================
# ì¶”ê°€: íŒŒì¼ ì—…ë¡œë“œ ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸ (ë¹„ë™ê¸°)
# ============================================================

@router.post("/file")
async def classify_file(file: UploadFile = File(...)):
    """
    íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ë¥˜ (ë¹„ë™ê¸° ë²„ì „)
    """
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        content = await file.read()
        text = content.decode('utf-8') if isinstance(content, bytes) else str(content)
        
        # ê¸°ë³¸ user_context (íŒŒì¼ ì—…ë¡œë“œë¼ user_id ì—†ìœ¼ë©´ anonymous)
        user_context = {
            "user_id": None,
            "file_id": file.filename,
            "occupation": "ì¼ë°˜ ì‚¬ìš©ì",
            "areas": [],
            "interests": [],
            "context_keywords": {}
        }

        # ë¶„ë¥˜ ì‹¤í–‰
        keyword_classifier = KeywordClassifier()
        keyword_result = await keyword_classifier.aclassify(
            text=text,
            user_context=user_context
        )

        new_keyword_tags = keyword_result.get('tags', ['ê¸°íƒ€'])
        logger.info(f"âœ… íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ: {file.filename}")
        logger.info(f"   - Tags: {new_keyword_tags}")

        return {
            "status": "success",
            "filename": file.filename,
            "keyword_tags": new_keyword_tags,
            "confidence": keyword_result.get('confidence', 0.0)
        }

    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
