# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# backend/routes/classifier_routes.py
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
í†µí•© ë¶„ë¥˜ ë¼ìš°í„° (Phase 3.1)

ì´ íŒŒì¼ì€ ëª¨ë“  ë¶„ë¥˜ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤:
- í•µì‹¬ ë¶„ë¥˜ (classify, file)
- ë©”íƒ€ë°ì´í„° ê´€ë¦¬ (save, metadata, saved)
- ê³ ê¸‰ ë¶„ë¥˜ (text, metadata, hybrid, parallel, para, keywords)
"""

import os
import json
import time
import csv
import requests
import time
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Optional, List, Any

from fastapi import FastAPI
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from pydantic import BaseModel


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í†µí•© ëª¨ë¸ Import
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
from backend.models import (
    ClassifyRequest,
    ClassifyResponse,
    ClassificationRequest,
    ClassificationResponse,
    ClassifyBatchRequest,
    ClassifyBatchResponse,
    MetadataClassifyRequest,
    HybridClassifyRequest,
    ParallelClassifyRequest,
    FileMetadata,
    SaveClassificationRequest,
    SearchRequest,
    HealthCheckResponse,
    MetadataResponse,
    ErrorResponse,
    SuccessResponse,
)

from backend.models.conflict import (
    ConflictRecord,
    ConflictReport
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ë¶„ë¥˜ ì—”ì§„ Import
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# í•¨ìˆ˜ ì„í¬íŠ¸
from backend.classifier.langchain_integration import (
    classify_with_langchain,
    classify_with_metadata,
    hybrid_classify
)
from backend.classifier.context_injector import get_context_injector
from backend.classifier.para_agent import run_para_agent
from backend.data_manager import save_json_log
save_func = save_json_log.__func__ if hasattr(save_json_log, "__func__") else save_json_log

# í´ë˜ìŠ¤ ì„í¬íŠ¸
from backend.services.conflict_service import conflict_service
from backend.data_manager import DataManager
from backend.classifier.keyword_classifier import KeywordClassifier
from backend.services.parallel_processor import ParallelClassifier
from backend.services.conflict_service import ConflictService
from backend.chunking import TextChunker
from backend.classifier.langchain_integration import PARAClassificationOutput
from backend.classifier.metadata_classifier import MetadataClassifier
from backend.classifier.para_classifier import PARAClassifier
from backend.classifier.context_injector import ContextInjector
from backend.database.metadata_schema import ClassificationMetadataExtender

import uuid
import logging

logger = logging.getLogger(__name__)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# API Router ì¶”ê°€
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
router = APIRouter()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

injector = get_context_injector() 

data_manager = DataManager()            # DataManager ì¸ìŠ¤í„´ìŠ¤

chunker = TextChunker(chunk_size=500, chunk_overlap=50)

SAVED_CLASSIFICATIONS = {}              # In-memory storage



# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# API ì—”ë“œí¬ì¸íŠ¸
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“Œ Section 1: Main API (2ê°œ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Tagë§Œ ì¶”ê°€
@router.post("/classify", response_model=ClassifyResponse, tags=["Classification", "Main API", "Text"])
async def classify_text(request: ClassifyRequest):
    """
    ë©”ì¸ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (KeywordClassifier + ConflictService)
    
    - ë§¤ë²ˆ ìƒˆë¡œìš´ KeywordClassifier ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    - ë¹„ë™ê¸° aclassify() ì‚¬ìš©
    - ì‚¬ìš©ì ë§¥ë½(occupation, areas, interests) ì™„ì „ ë°˜ì˜
    - ìƒˆ keyword_tags ë§¤ë²ˆ ìƒì„±
    - DB ë° ë¡œê·¸ì— ì €ì¥
    
    Example:
        POST /api/classifier/classify
        {
            "text": "í”„ë¡œì íŠ¸ ì™„ì„±í•˜ê¸°",
            "user_id": "user_123",
            "occupation": "ê°œë°œì",
            "areas": ["ë°±ì—”ë“œ", "AI"],
            "interests": ["ë¨¸ì‹ ëŸ¬ë‹"]
        }
    """
    try:
        logger.info(f"ğŸ“ ë¶„ë¥˜ ìš”ì²­ ì‹œì‘:")
        logger.info(f"   - Text: {request.text[:50]}...")
        logger.info(f"   - User ID: {request.user_id}")
        logger.info(f"   - Occupation: {request.occupation}")
        logger.info(f"   - Areas: {request.areas}")
        logger.info(f"   - Areas: {request.areas}")
        logger.info(f"   - Interests: {request.interests}")
        
        
        # ============================================================
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ============================================================
        
        user_context = {
            "user_id": request.user_id,
            "file_id": request.file_id,
            "occupation": request.occupation or "ì¼ë°˜ ì‚¬ìš©ì",      # ì§ì—…
            "areas": request.areas,                              # ì˜ì—­
            "interests": request.interests,                      # ê´€ì‹¬ì‚¬
            "context_keywords": {                                # ìë™ ìƒì„±
                area: [area, f"{area} ê´€ë ¨", f"{area} ì—…ë¬´", f"{area} í”„ë¡œì íŠ¸"]
                for area in (request.areas or [])
            }
        }
        
        logger.info(f"âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±:")
        logger.info(f"   - Occupation: {user_context['occupation']}")
        logger.info(f"   - Areas: {user_context['areas']}")
        logger.info(f"   - Context Keywords: {list(user_context['context_keywords'].keys())}")
        
        
        # ============================================================
        # Step 2: PARA ë¶„ë¥˜
        # ============================================================
        try:
            # PARA Agent ì‹¤í–‰
            para_result = await run_para_agent(
                text=request.text,
                metadata={
                    "user_id": request.user_id,
                    "file_id": request.file_id,
                    "occupation": request.occupation,
                    "areas": request.areas,
                    "interests": request.interests          # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
                }
            )
            logger.info(f"âœ… PARA ë¶„ë¥˜ ì™„ë£Œ:")
            logger.info(f"   - Category: {para_result.get('category')}")
            logger.info(f"   - Confidence: {para_result.get('confidence')}")
            logger.info(f"   - Snapshot ID: {para_result.get('snapshot_id')}")
            
        except Exception as para_error:
            logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {para_error}", exc_info=True)
            # ê¸°ë³¸ê°’ ì„¤ì •
            para_result = {
                "category": "Resources",
                "confidence": 0.0,
                "snapshot_id": f"snap_failed_{int(datetime.now().timestamp())}"
            }
        
        
        # ============================================================
        # Step 3: í‚¤ì›Œë“œ ì¶”ì¶œ
        # ============================================================
        keyword_classifier = KeywordClassifier()
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ (Instance ID: {keyword_classifier.instance_id})")
        
        keyword_result = await keyword_classifier.aclassify(
            text=request.text,
            user_context=user_context
        )
        
        # í‚¤ì›Œë“œ ì•ˆì „ ì²˜ë¦¬
        new_keyword_tags = keyword_result.get('tags', ['ê¸°íƒ€'])
        if not isinstance(new_keyword_tags, list):
            new_keyword_tags = [str(new_keyword_tags)] if new_keyword_tags else ['ê¸°íƒ€']
        else:
            new_keyword_tags = [str(tag) for tag in new_keyword_tags if str(tag).strip()]
            if not new_keyword_tags:
                new_keyword_tags = ['ê¸°íƒ€']

        logger.info(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"   - Instance ID: {keyword_result.get('instance_id')}")
        logger.info(f"   - Tags: {new_keyword_tags[:5]}")
        logger.info(f"   - Confidence: {keyword_result.get('confidence')}")
        logger.info(f"   - User Context Matched: {keyword_result.get('user_context_matched')}")
        logger.info(f"   - Processing Time: {keyword_result.get('processing_time')}")
        
        
        # ============================================================
        # Step 4: ì¶©ëŒ í•´ê²°
        # ============================================================
        conflict_service = ConflictService()
        
        conflict_result = await conflict_service.classify_text(
            para_result=para_result,
            keyword_result=keyword_result,
            text=request.text,
            user_context=user_context                                   # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
        )
        
        logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ:")
        logger.info(f"   - Final Category: {conflict_result.get('final_category')}")
        logger.info(f"   - Keyword Tags: {conflict_result.get('keyword_tags', new_keyword_tags)}")
        logger.info(f"   - Conflict Detected: {conflict_result.get('conflict_detected')}")
        logger.info(f"   - Requires Review: {conflict_result.get('requires_review')}")


        # ============================================================
        # Step 5: ìµœì¢… ì¹´í…Œê³ ë¦¬ ê²°ì • + ë¡œê·¸ ì €ì¥ + ì‘ë‹µ ë°˜í™˜ (ì™„ë²½ ì •ë¦¬íŒ)
        # ============================================================

        # 1. ìµœì¢… ì¹´í…Œê³ ë¦¬ ê²°ì • (ì´ ì¤„ì´ ì œì¼ ì¤‘ìš”!)
        final_category = (
            conflict_result.get("final_category")
            or para_result.get("category")
            or "Resources"
        )

        # 2. DataManagerë¡œ CSV ë¡œê·¸ ê¸°ë¡ (ê¸°ì¡´ì— ìˆë˜ ê±° ì¬ì‚¬ìš©)
        csv_log_result = {}
        try:
            csv_log_result = data_manager.log_classification(
                user_id=request.user_id or "anonymous",
                file_name=request.file_id or "text_input",
                ai_prediction=final_category,
                user_selected=None,
                confidence=conflict_result.get("confidence", 0.0)
            )
        except Exception as e:
            logger.warning(f"DataManager CSV ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        # 3. í†µí•© ë¡œê·¸ ì €ì¥ (CSV + JSON + ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸)
        try:
            from pathlib import Path
            import json
            import csv
            from datetime import datetime as dt

            PROJECT_ROOT = Path(__file__).parent.parent.parent
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            CSV_DIR = PROJECT_ROOT / "data" / "classifications"
            CTX_DIR = PROJECT_ROOT / "data" / "context"

            for d in (LOG_DIR, CSV_DIR, CTX_DIR):
                d.mkdir(parents=True, exist_ok=True)

            timestamp = dt.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

            # 3-1. CSV ë°±ì—… ì €ì¥
            csv_path = CSV_DIR / "classification_log.csv"
            file_exists = csv_path.exists()
            with open(csv_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=["timestamp","user_id","file_id","category","confidence","keyword_tags"])
                if not file_exists:
                    writer.writeheader()
                writer.writerow({
                    "timestamp": dt.now().isoformat(),
                    "user_id": request.user_id or "anonymous",
                    "file_id": request.file_id or "text_input",
                    "category": final_category,
                    "confidence": round(conflict_result.get("confidence", 0.0), 3),
                    "keyword_tags": ",".join(new_keyword_tags)
                })

            # 3-2. JSON ë¡œê·¸ ì €ì¥
            json_path = LOG_DIR / f"classification_{timestamp}.json"
            json_log_data = {
                "timestamp": timestamp,
                "user_id": request.user_id or "anonymous",
                "text_preview": request.text[:100],
                "final_category": final_category,
                "keyword_tags": new_keyword_tags,
                "confidence": conflict_result.get("confidence", 0.0),
                "snapshot_id": str(para_result.get("snapshot_id", "unknown")),
                "user_areas": request.areas or [],
                "context_matched": keyword_result.get("user_context_matched", False)
            }
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_log_data, f, ensure_ascii=False, indent=2)

            # 3-3. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ì €ì¥
            ctx_path = CTX_DIR / "user_context_mapping.json"
            ctx_data = {}
            if ctx_path.exists():
                try:
                    with open(ctx_path, "r", encoding="utf-8") as f:
                        ctx_data = json.load(f)
                except:
                    ctx_data = {}

            uid = request.user_id or "anonymous"
            ctx_data.setdefault(uid, {"occupation": request.occupation or "ì¼ë°˜ ì‚¬ìš©ì", "areas": [], "interests": [], "recent_categories": [], "total_classifications": 0})
            ctx_data[uid]["recent_categories"].append(final_category)
            ctx_data[uid]["recent_categories"] = ctx_data[uid]["recent_categories"][-10:]
            ctx_data[uid]["total_classifications"] += 1
            ctx_data[uid]["last_updated"] = dt.now().isoformat()

            with open(ctx_path, "w", encoding="utf-8") as f:
                json.dump(ctx_data, f, ensure_ascii=False, indent=2)

            log_info = {
                "csv_log": str(csv_path),
                "json_log": json_path.name,
                "context_saved": True,
                "log_directory": str(LOG_DIR)
            }

        except Exception as e:
            logger.warning(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
            log_info = {"error": str(e)}

        # 4. ìµœì¢… ì‘ë‹µ
        response = ClassifyResponse(
            category=final_category,
            confidence=conflict_result.get("confidence", para_result.get("confidence", 0.0)),
            snapshot_id=str(para_result.get("snapshot_id", "")),
            conflict_detected=conflict_result.get("conflict_detected", False),
            requires_review=conflict_result.get("requires_review", False),
            keyword_tags=new_keyword_tags,
            reasoning=conflict_result.get("reason", ""),
            user_context_matched=keyword_result.get("user_context_matched", False),
            user_areas=request.areas or [],
            user_context=user_context,
            context_injected=bool(request.areas),
            log_info=log_info,
            csv_log_result=csv_log_result
        )

        logger.info(f"âœ… ì „ì²´ ë¶„ë¥˜ ì™„ë£Œ â†’ {response.category} | í‚¤ì›Œë“œ {len(response.keyword_tags)}ê°œ")
        logger.info(f"   - Final Category: {response.category}")
        logger.info(f"   - Keyword Tags: {response.keyword_tags[:3]}...")
        logger.info(f"   - User Context Matched: {response.user_context_matched}")
        logger.info(f"   - Total Time: ~{keyword_result.get('processing_time', 'N/A')}")

        return response

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")


# classify_file â†’ classify_file_main
@router.post("/file", response_model=ClassifyResponse, tags=["Classification", "Main API", "File Upload"])
async def classify_file_main(
    request: Request,
    file: UploadFile = File(...),
    user_id: Optional[str] = Form(None),
    file_id: Optional[str] = Form(None),
    occupation: Optional[str] = Form(None),
    areas: Optional[str] = Form(None),
    interests: Optional[str] = Form(None),
    selected_category: Optional[str] = Form(None)
):
    """
    ë©”ì¸ íŒŒì¼ ë¶„ë¥˜ (classify_text ì¬ì‚¬ìš©)
    
    - íŒŒì¼ ì—…ë¡œë“œ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - classify_textì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
    - Form ë°ì´í„°ë¡œ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
    
    Example:
        POST /api/classifier/file
        Content-Type: multipart/form-data
        
        file: test.txt
        user_id: user_123
        occupation: ê°œë°œì
        areas: ["ë°±ì—”ë“œ", "AI"]
    """
    try:
        import json
        from pathlib import Path
        import csv
        import time

        # === ì´ˆê¸° ìƒíƒœê°’ ë³´ì¥ ===
        csv_saved = False
        csv_direct_saved = False
        json_saved = False
        context_saved = False
        csv_log_result = None
        json_filename = None
        areas_list = None
        
        # ============================================================
        # Step 1: íŒŒì¼ ì½ê¸° + Form ë°ì´í„° íŒŒì‹±
        # ============================================================
        content = await file.read()
        text = content.decode("utf-8", errors="ignore") if isinstance(content, (bytes, bytearray)) else str(content)

        # JSON stringì„ listë¡œ ë³€í™˜ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        try:
            areas_list = json.loads(areas) if areas else []
        except Exception:
            areas_list = []
        try:
            interests_list = json.loads(interests) if interests else []
        except Exception:
            interests_list = []

        logger.info(f"ğŸ“‚ íŒŒì¼ ì½ê¸°: {file.filename}")

        # ============================================================
        # Step 2: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ============================================================
        # ìš°ì„  Formìœ¼ë¡œ ì „ë‹¬ëœ user_idë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ request.stateì—ì„œ ì°¾ìŒ
        effective_user_id = user_id or getattr(request.state, "user_id", None) or "anonymous"
        
        # âœ… DataManagerì—ì„œ ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ
        data_manager = DataManager()
        user_profile = data_manager.get_user_profile(effective_user_id)
        
        # í”„ë¡œí•„ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ Form ë°ì´í„° ì‚¬ìš©
        if user_profile:
            occupation_value = user_profile.get("occupation", "ì¼ë°˜ ì‚¬ìš©ì")
            # CSVì—ì„œ ì½ì€ areasëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            stored_areas = user_profile.get("areas", "")
            areas_list = [a.strip() for a in stored_areas.split(",")] if stored_areas else areas_list or []
            stored_interests = user_profile.get("interests", "")
            interests_list = [i.strip() for i in stored_interests.split(",")] if stored_interests else interests_list or []
            logger.info(f"âœ… í”„ë¡œí•„ ë¡œë“œ ì„±ê³µ: {occupation_value}, areas={len(areas_list)}ê°œ")
        else:
            occupation_value = occupation or "ì¼ë°˜ ì‚¬ìš©ì"
            logger.warning(f"âš ï¸ í”„ë¡œí•„ ì—†ìŒ, Form ë°ì´í„° ì‚¬ìš©")

        user_context = {
            "user_id": effective_user_id,
            "file_id": file_id or file.filename,
            "occupation": occupation_value,
            "areas": areas_list,
            "interests": interests_list,
            "context_keywords": {
                area: [area, f"{area} ê´€ë ¨", f"{area} ì—…ë¬´", f"{area} í”„ë¡œì íŠ¸"]
                for area in areas_list
            } if areas_list else {}
        }

        logger.info(f"ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...:")
        logger.info(f"   - Occupation: {user_context['occupation']}")
        logger.info(f"   - Areas: {user_context['areas']}")
        logger.info(f"   - Context Keywords: {list(user_context['context_keywords'].keys())}")
        
        logger.info(f"ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (user_id={effective_user_id})")

        # ============================================================
        # Step 3: PARA ë¶„ë¥˜
        # ============================================================
        try:
            para_result = await run_para_agent(
                text=text,
                metadata={
                    "user_id": effective_user_id,
                    "file_id": file_id or file.filename,
                    "occupation": occupation,
                    "areas": areas_list,
                    "interests": interests_list
                    #"user_id": request.user_id,
                    #"file_id": request.file_id,
                    #"occupation": request.occupation,
                    #"areas": request.areas,
                    #"interests": request.interests          # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬            
                }
            )
            logger.info(f"âœ… PARA ë¶„ë¥˜ ì™„ë£Œ:")
            logger.info(f"   - Category: {para_result.get('category')}")
            logger.info(f"   - Confidence: {para_result.get('confidence')}")
            logger.info(f"   - Snapshot ID: {para_result.get('snapshot_id')}")
            
        except Exception as para_error:
            logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {para_error}", exc_info=True)
            para_result = {
                "category": "Resources",
                "confidence": 0.0,
                "snapshot_id": f"snap_failed_{int(datetime.now(timezone.utc).timestamp())}"
            }

        # ============================================================
        # Step 4: í‚¤ì›Œë“œ ì¶”ì¶œ
        # ============================================================

        keyword_classifier = KeywordClassifier()                # ë§¤ë²ˆ ìƒˆ ì¸ìŠ¤í„´ìŠ¤!

        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ (Instance ID: {keyword_classifier.instance_id})")

        # âœ… ìˆ˜ì •: aclassify í˜¸ì¶œ í›„ ì•ˆì „í•˜ê²Œ tags ì¶”ì¶œ
        keyword_result = await keyword_classifier.aclassify(
            text=text,
            user_context=user_context
        )

        # âœ… í•µì‹¬ ìˆ˜ì •: tags ì¶”ì¶œ ë¡œì§ ê°•í™”
        raw_tags = keyword_result.get('tags', [])
        logger.info(f"ğŸ“¦ Raw tags from LLM: {raw_tags} (type: {type(raw_tags)})")

        # 1. Noneì´ê±°ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if not raw_tags:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  Tagsê°€ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {new_keyword_tags}")
            
        # 2. ë¬¸ìì—´ì¸ ê²½ìš° (LLMì´ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë¬¸ìì—´ë¡œ ë°˜í™˜í•œ ê²½ìš°)
        elif isinstance(raw_tags, str):
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì¸ì§€ í™•ì¸
            if ',' in raw_tags:
                new_keyword_tags = [tag.strip() for tag in raw_tags.split(',') if tag.strip()]
            else:
                new_keyword_tags = [raw_tags.strip()] if raw_tags.strip() else ['ê¸°íƒ€']
            logger.info(f"âœ… ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜: {new_keyword_tags}")

        # 3. ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì •ìƒ)
        elif isinstance(raw_tags, list):
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ëª¨ë“  ìš”ì†Œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
            valid_tags = [str(tag).strip() for tag in raw_tags if tag and str(tag).strip()]
            new_keyword_tags = valid_tags if valid_tags else ['ê¸°íƒ€']
            logger.info(f"âœ… ë¦¬ìŠ¤íŠ¸ ê²€ì¦ ì™„ë£Œ: {len(valid_tags)}ê°œ íƒœê·¸")

        # 4. ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…
        else:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… {type(raw_tags)}, ê¸°ë³¸ê°’ ì‚¬ìš©")

        # âœ… ìµœì¢… ê²€ì¦: ìµœì†Œ 1ê°œ ì´ìƒì˜ íƒœê·¸ ë³´ì¥
        if not new_keyword_tags or len(new_keyword_tags) == 0:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  ìµœì¢… ê²€ì¦ ì‹¤íŒ¨, ê°•ì œ ê¸°ë³¸ê°’ ì„¤ì •")

        logger.info(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"   - Instance ID: {keyword_result.get('instance_id')}")
        logger.info(f"   - Final Tags: {new_keyword_tags[:5]}")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
        logger.info(f"   - Tags Count: {len(new_keyword_tags)}")
        logger.info(f"   - Confidence: {keyword_result.get('confidence')}")
        logger.info(f"   - User Context Matched: {keyword_result.get('user_context_matched')}")
        logger.info(f"   - Processing Time: {keyword_result.get('processing_time')}")

        # ============================================================
        # Step 5: ì¶©ëŒ í•´ê²°
        # ============================================================
        conflict_service = ConflictService()
        conflict_result = await conflict_service.classify_text(
            para_result=para_result,
            keyword_result=keyword_result,
            text=text,
            user_context=user_context
        )

        logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ: {conflict_result.get('final_category')}")


        # ============================================================
        # Step 6: ìµœì¢… ì¹´í…Œê³ ë¦¬ ê²°ì • + ë¡œê·¸ ì €ì¥ + ì‘ë‹µ ë°˜í™˜ (ì™„ë²½ ì •ë¦¬íŒ)
        # ============================================================

        # 1. ìµœì¢… ì¹´í…Œê³ ë¦¬ ê²°ì • (ì´ ì¤„ì´ ì œì¼ ì¤‘ìš”!)
        final_category = (
            conflict_result.get("final_category")
            or para_result.get("category")
            or "Resources"
        )

        # 2. DataManagerë¡œ CSV ë¡œê·¸ ê¸°ë¡ (ê¸°ì¡´ì— ìˆë˜ ê±° ì¬ì‚¬ìš©)
        csv_log_result = {}
        try:
            csv_log_result = data_manager.log_classification(
                user_id=request.user_id or "anonymous",
                file_name=request.file_id or "text_input",
                ai_prediction=final_category,
                user_selected=None,
                confidence=conflict_result.get("confidence", 0.0)
            )
        except Exception as e:
            logger.warning(f"DataManager CSV ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        # 3. í†µí•© ë¡œê·¸ ì €ì¥ (CSV + JSON + ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸)
        try:
            from pathlib import Path
            import json
            import csv
            from datetime import datetime as dt

            PROJECT_ROOT = Path(__file__).parent.parent.parent
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            CSV_DIR = PROJECT_ROOT / "data" / "classifications"
            CTX_DIR = PROJECT_ROOT / "data" / "context"

            for d in (LOG_DIR, CSV_DIR, CTX_DIR):
                d.mkdir(parents=True, exist_ok=True)

            timestamp = dt.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]

            # 3-1. CSV ë°±ì—… ì €ì¥
            csv_path = CSV_DIR / "classification_log.csv"
            file_exists = csv_path.exists()
            with open(csv_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=["timestamp","user_id","file_id","category","confidence","keyword_tags"])
                if not file_exists:
                    writer.writeheader()
                writer.writerow({
                    "timestamp": dt.now().isoformat(),
                    "user_id": request.user_id or "anonymous",
                    "file_id": request.file_id or "text_input",
                    "category": final_category,
                    "confidence": round(conflict_result.get("confidence", 0.0), 3),
                    "keyword_tags": ",".join(new_keyword_tags)
                })

            # 3-2. JSON ë¡œê·¸ ì €ì¥
            json_path = LOG_DIR / f"classification_{timestamp}.json"
            json_log_data = {
                "timestamp": timestamp,
                "user_id": request.user_id or "anonymous",
                "text_preview": request.text[:100],
                "final_category": final_category,
                "keyword_tags": new_keyword_tags,
                "confidence": conflict_result.get("confidence", 0.0),
                "snapshot_id": str(para_result.get("snapshot_id", "unknown")),
                "user_areas": request.areas or [],
                "context_matched": keyword_result.get("user_context_matched", False)
            }
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_log_data, f, ensure_ascii=False, indent=2)

            # 3-3. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ì €ì¥
            ctx_path = CTX_DIR / "user_context_mapping.json"
            ctx_data = {}
            if ctx_path.exists():
                try:
                    with open(ctx_path, "r", encoding="utf-8") as f:
                        ctx_data = json.load(f)
                except:
                    ctx_data = {}

            uid = request.user_id or "anonymous"
            ctx_data.setdefault(uid, {"occupation": request.occupation or "ì¼ë°˜ ì‚¬ìš©ì", "areas": [], "interests": [], "recent_categories": [], "total_classifications": 0})
            ctx_data[uid]["recent_categories"].append(final_category)
            ctx_data[uid]["recent_categories"] = ctx_data[uid]["recent_categories"][-10:]
            ctx_data[uid]["total_classifications"] += 1
            ctx_data[uid]["last_updated"] = dt.now().isoformat()

            with open(ctx_path, "w", encoding="utf-8") as f:
                json.dump(ctx_data, f, ensure_ascii=False, indent=2)

            log_info = {
                "csv_log": str(csv_path),
                "json_log": json_path.name,
                "context_saved": True,
                "log_directory": str(LOG_DIR)
            }

        except Exception as e:
            logger.warning(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
            log_info = {"error": str(e)}

        # 4. ìµœì¢… ì‘ë‹µ
        response = ClassifyResponse(
            category=final_category,
            confidence=conflict_result.get("confidence", para_result.get("confidence", 0.0)),
            snapshot_id=str(para_result.get("snapshot_id", "")),
            conflict_detected=conflict_result.get("conflict_detected", False),
            requires_review=conflict_result.get("requires_review", False),
            keyword_tags=new_keyword_tags,
            reasoning=conflict_result.get("reason", ""),
            user_context_matched=keyword_result.get("user_context_matched", False),
            user_areas=request.areas or [],
            user_context=user_context,
            context_injected=bool(request.areas),
            log_info=log_info,
            csv_log_result=csv_log_result
        )

        logger.info(f"âœ… ì „ì²´ ë¶„ë¥˜ ì™„ë£Œ â†’ {response.category} | í‚¤ì›Œë“œ {len(response.keyword_tags)}ê°œ")
        logger.info(f"   - Final Category: {response.category}")
        logger.info(f"   - Keyword Tags: {response.keyword_tags[:3]}...")
        logger.info(f"   - User Context Matched: {response.user_context_matched}")
        logger.info(f"   - Total Time: ~{keyword_result.get('processing_time', 'N/A')}")

        return response

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“¦ Section 2: Advanced API (4ê°œ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ê³ ê¸‰ ë¶„ë¥˜ (ê¸°ì¡´ api_routes.pyì˜ ë¶„ë¥˜ ë¡œì§)

# classify_file â†’ classify_file_advanced
# "/classied/file" â†’ "/advanced/file"
@router.post("/advanced/file", tags=["Classification", "Advanced", "LangGraph"])
async def classify_file_advanced(file: UploadFile = File(...)):
    """ê³ ê¸‰ íŒŒì¼ ë¶„ë¥˜ (LangGraph + ë©”íƒ€ë°ì´í„° ì €ì¥)"""
    try:
        # 1. íŒŒì¼ ì½ê¸°
        content = await file.read()
        text = content.decode('utf-8')
        filename = file.filename
        
        logger.info(f"ğŸš€ ë¶„ë¥˜ ì‹œì‘: {filename}")
        
        # 2. ì²­í‚¹
        chunks = chunker.chunk_text(text)
        chunk_count = len(chunks)
        
        # 3. íŒŒì¼ ID ìƒì„± (UUID)
        file_id = f"file_{uuid.uuid4().hex[:8]}"
        
        # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
        try:
            data_manager.add_file(
                file_name=filename,
                file_size=len(content),
                chunk_count=chunk_count,
                embedding_dim=1536,
                model="text-embedding-3-small"
            )
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {file_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # 5. LangGraph ê¸°ë°˜ ê³ ë„í™” ë¶„ë¥˜
        metadata = {
            "filename": filename,
            "file_size": len(content),
            "chunk_count": chunk_count,
            "uploaded_at": datetime.now().isoformat()
        }
        
        # ì²˜ìŒ 2000ìë§Œ ë¶„ë¥˜ (ë¹„ìš© ì ˆê°)
        sample_text = text[:2000]
        
        # ğŸ”¥ Sync ë²„ì „ í˜¸ì¶œ!
        try:
            para_result = await run_para_agent(
                text=sample_text,
                metadata=metadata
            )
            logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: {para_result['category']}")
        except Exception as e:
            logger.error(f"âŒ LangGraph ì—ëŸ¬: {e}")
            # Fallback
            para_result = {
                "category": "Resources",
                "keyword_tags": sample_text.split()[:10],
                "confidence": 0.5,
                "conflict_detected": False
            }
        
        # 6. ì‘ë‹µ ìƒì„±
        response = {
            "final_category": para_result.get('category', 'Resources'),
            "para_category": para_result.get('category', 'Resources'),
            "keyword_tags": para_result.get('keyword_tags', [])[:10],  # ìƒìœ„ 10ê°œë§Œ
            "confidence": para_result.get('confidence', 0.5),
            "confidence_gap": para_result.get('confidence_gap', 0.0),
            "conflict_detected": para_result.get('conflict_detected', False),
            "resolution_method": para_result.get('resolution_method', 'auto'),
            "requires_review": para_result.get('requires_review', False),
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            "metadata": {
                "file_id": file_id,
                "filename": filename,
                "chunk_count": chunk_count,
                "file_size_kb": round(len(content) / 1024, 2),
                "text_preview": sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
            }
        }
        return response
        
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜. UTF-8 íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ ì—ëŸ¬: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.post("/save-classification", response_model=SuccessResponse, tags=["Classification", "Storage", "Save"])
async def save_classification(request: SaveClassificationRequest):
    """ë¶„ë¥˜ ê²°ê³¼ ì €ì¥"""
    try:
        # 1. ì‹¤ì œ ì €ì¥ (ê²½ë¡œ ë°›ìŒ)
        saved_path = save_func(
            user_id="system",  # ë˜ëŠ” request.user_id ìˆìœ¼ë©´ ê·¸ê±¸ ì“°ì„¸ìš”
            file_name=request.file_id,
            category=request.classification.get("category", "Unknown"),
            confidence=request.classification.get("confidence", 0.0),
            snapshot_id="manual_save",
            conflict_detected=False,
            requires_review=False,
            keyword_tags=request.classification.get("keyword_tags", []),
            reasoning="ì‚¬ìš©ìê°€ ì§ì ‘ ì €ì¥í•œ ë¶„ë¥˜ ê²°ê³¼",
            user_context="",
            user_profile=None,
            context_injected=False
        )
        
        logger.info(f"ğŸ’¾ ì €ì¥ë¨: {request.file_id} â†’ {saved_path}")
        
        return {
            "status": "saved",
            "file_id": request.file_id,
            "saved_path": saved_path,                    # ë³´ë„ˆìŠ¤: ì‹¤ì œ ì €ì¥ ìœ„ì¹˜ ì•Œë ¤ì¤Œ
            "message": "ë¶„ë¥˜ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        logger.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.get("/saved-files", tags=["Classification", "Storage", "List"])
async def get_saved_files():
    """ì €ì¥ëœ íŒŒì¼ ëª©ë¡"""
    return SAVED_CLASSIFICATIONS

# Tagë§Œ ì¶”ê°€
@router.get("/metadata/{file_id}", response_model=Dict, tags=["Classification", "Metadata", "Query"])
async def get_metadata(file_id: str):
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    try:
        metadata = data_manager.get_file(file_id)
        if not metadata:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return metadata
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ”¬ Section 3: Specialized Methods (6ê°œ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ê³ ê¸‰ ë¶„ë¥˜ ë©”ì„œë“œ (ê¸°ì¡´ classifier_routes.py)

# classify_text_endpoint â†’ classify_text_langchain
# Tag ì¶”ê°€
@router.post("/text", tags=["Classification", "Advanced", "LangChain Only"])
async def classify_text_langchain(request: ClassificationRequest):
    """
    ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ (LangChain ê¸°ë°˜)
    
    - ê°œë°œì/í…ŒìŠ¤íŠ¸ìš©
    - LangChainë§Œ ì‚¬ìš©, ì¶©ëŒ í•´ê²° ì—†ìŒ
    """
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ (LangChain ê¸°ë°˜)"""
    try:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (classify_with_langchain ë“±)
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        user_areas = []
        if request.user_id:
            try:
                user_context = data_manager.get_user_context(request.user_id)
                user_areas = user_context.get('areas', [])
                
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                logger.info(f"ğŸ“„ ìƒˆ ë¶„ë¥˜ ìš”ì²­")
                logger.info(f"User ID: {request.user_id}")
                logger.info(f"User Areas: {user_areas}")
                logger.info(f"Text Preview: {request.text[:100]}...")
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            except Exception as e:
                logger.warning(f"âš ï¸ Context loading failed: {e}")
        
        # Step 2: AI ë¶„ì„ (LangChain ì‚¬ìš©)
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•¨!
        result = classify_with_langchain(request.text)
        
        # Step 3: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id and user_areas:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
            result["context_injected"] = True
            result["user_areas"] = user_areas
        else:
            result["context_injected"] = False
            result["user_areas"] = []

        # Step 4: ë””ë²„ê¹… ë¡œê·¸
        logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"  - Category: {result.get('category', 'N/A')}")
        logger.info(f"  - Tags: {result.get('tags', [])[:5]}")
        logger.info(f"  - Context Injected: {result.get('context_injected', False)}")
        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id="",  # ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ëŠ” ìŠ¤ëƒ…ìƒ· ì—†ìŒ
            conflict_detected=False,
            requires_review=False,
            user_context_matched=result.get("context_injected", False),
            user_areas=result.get("user_areas", []),
            user_context={},  # í•„ìš”í•˜ë©´ ì±„ìš°ê¸°
            context_injected=result.get("context_injected", False),
            log_info={"source": "metadata"},
            csv_log_result={}
        )
    
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.post("/metadata", response_model=ClassifyResponse, tags=["Classification", "Advanced", "Metadata Based"])
async def classify_metadata_endpoint(request: MetadataClassifyRequest):
    """ë©”íƒ€ë°ì´í„° ë¶„ë¥˜"""
    try:
        result = classify_with_metadata(request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id="",             # ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ëŠ” ìŠ¤ëƒ…ìƒ· ì—†ìŒ
            conflict_detected=False,
            requires_review=False,
            user_context_matched=result.get("context_injected", False),
            user_areas=result.get("user_areas", []),
            user_context={},            # í•„ìš”í•˜ë©´ ì±„ìš°ê¸°
            context_injected=result.get("context_injected", False),
            log_info={"source": "metadata"},
            csv_log_result={}
        )
    
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.post("/hybrid", response_model=ClassifyResponse, tags=["Classification", "Advanced", "Hybrid"])
async def hybrid_classify_endpoint(request: HybridClassifyRequest):
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ (í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„°)"""
    try:
        result = hybrid_classify(request.text, request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id="",  # ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ëŠ” ìŠ¤ëƒ…ìƒ· ì—†ìŒ
            conflict_detected=False,
            requires_review=False,
            user_context_matched=result.get("context_injected", False),
            user_areas=result.get("user_areas", []),
            user_context={},  # í•„ìš”í•˜ë©´ ì±„ìš°ê¸°
            context_injected=result.get("context_injected", False),
            log_info={"source": "metadata"},
            csv_log_result={}
        )
    
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.post("/parallel", tags=["Classification", "Advanced", "Parallel"])
async def parallel_classify_endpoint(request: ParallelClassifyRequest):
    """í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° ë³‘ë ¬ ë¶„ë¥˜"""
    try:
        # ë³‘ë ¬ ì²˜ë¦¬
        result = ParallelClassifier.classify_parallel(
            text=request.text,
            metadata=request.metadata or {}
        )
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("keyword_tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id=result.get("snapshot_id", ""),
            conflict_detected=result.get("conflict_detected", False),
            requires_review=result.get("requires_review", False),
            user_context_matched=result.get("user_context_matched", False),
            user_areas=result.get("user_areas", []),
            user_context=result.get("user_context", {}),
            context_injected=result.get("context_injected", False),
            log_info=result.get("log_info", {"source": "parallel"}),
            csv_log_result=result.get("csv_log_result", {})
        )
    
    except Exception as e:
        logger.error(f"ë³‘ë ¬ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return ClassifyResponse(
            category="Resources",
            confidence=0.0,
            keyword_tags=[],
            reasoning="ë³‘ë ¬ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            log_info={"error": str(e)},
            csv_log_result={}
        )


# Tagë§Œ ì¶”ê°€
@router.post("/para", tags=["Classification", "Specialized", "PARA"])
async def classify_para(request: ClassificationRequest):
    """PARA ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
        - /classify/para ë¡œ ì ‘ê·¼ ê°€ëŠ¥
    """
    try:
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id="",  # ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ëŠ” ìŠ¤ëƒ…ìƒ· ì—†ìŒ
            conflict_detected=False,
            requires_review=False,
            user_context_matched=result.get("context_injected", False),
            user_areas=result.get("user_areas", []),
            user_context={},  # í•„ìš”í•˜ë©´ ì±„ìš°ê¸°
            context_injected=result.get("context_injected", False),
            log_info={"source": "metadata"},
            csv_log_result={}
        )
    
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Tagë§Œ ì¶”ê°€
@router.post("/keywords", tags=["Classification", "Specialized", "Keywords"])
async def classify_keywords(request: ClassificationRequest):
    """í‚¤ì›Œë“œ ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
        - ì ‘ê·¼: POST http://localhost:8000/classify/keywords
    """
    try:
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ìš”ì²­: {request.text[:50]}...")
        
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ!
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return ClassifyResponse(
            category=result.get("category", "Resources"),
            confidence=result.get("confidence", 0.0),
            keyword_tags=result.get("tags", []),
            reasoning=result.get("reasoning", ""),
            snapshot_id="",  # ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ëŠ” ìŠ¤ëƒ…ìƒ· ì—†ìŒ
            conflict_detected=False,
            requires_review=False,
            user_context_matched=result.get("context_injected", False),
            user_areas=result.get("user_areas", []),
            user_context={},  # í•„ìš”í•˜ë©´ ì±„ìš°ê¸°
            context_injected=result.get("context_injected", False),
            log_info={"source": "metadata"},
            csv_log_result={}
        )
    
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================================
# Section 4: History (1ê°œ)
# ========================================

# Tag ì¶”ê°€
# "/classify/snapshots" â†’ "snapshots" 
@router.get("/snapshots", tags=["Classification", "History", "Query"])
async def get_snapshots():
    """ì €ì¥ëœ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ"""
    return {"snapshots": conflict_service.get_snapshots()}
