# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# backend/routes/classifier_routes.py
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
ë¶„ë¥˜ ë¼ìš°íŠ¸
- LangChain ê¸°ë°˜ ë¶„ë¥˜
- ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜s
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
"""
import os
from pathlib import Path
import json
import time
import requests

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from pydantic import BaseModel
from typing import Dict, Optional, List, Any
from datetime import datetime, timezone

# í•¨ìˆ˜ ì„í¬íŠ¸
from backend.classifier.langchain_integration import (
    classify_with_langchain,
    classify_with_metadata,
    hybrid_classify
)
from backend.classifier.context_injector import get_context_injector
from backend.classifier.para_agent import run_para_agent


# í´ë˜ìŠ¤ ì„í¬íŠ¸
from backend.classifier.langchain_integration import PARAClassificationOutput
from backend.services.parallel_processor import ParallelClassifier
from backend.classifier.keyword_classifier import KeywordClassifier
from backend.classifier.metadata_classifier import MetadataClassifier
from backend.classifier.para_classifier import PARAClassifier
from backend.classifier.context_injector import ContextInjector
from backend.services.conflict_service import ConflictService
from backend.routes.conflict_routes import ClassifyRequest, ClassifyResponse
from backend.metadata import FileMetadata
from backend.classifier.keyword_classifier import KeywordClassifier
from backend.data_manager import DataManager
from backend.database.metadata_schema import ClassificationMetadataExtender
# ëª¨ë¸ ì„í¬íŠ¸
from backend.models.classification import (
    ClassifyRequest,
    ClassifyResponse,
    ClassificationRequest,
    ClassificationResponse,
    MetadataClassifyRequest,
    HybridClassifyRequest,
    ParallelClassifyRequest
)


import logging

logger = logging.getLogger(__name__)


# ============ Router ì´ˆê¸°í™” ============
router = APIRouter()                    # API Router ì¶”ê°€


# ============ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ============
# ìš”ì²­ë§ˆë‹¤ ì¬ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
injector = get_context_injector()


# ============ API ì—”ë“œí¬ì¸íŠ¸ ============

@router.post("/text")
async def classify_text_endpoint(request: ClassificationRequest):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ (LangChain ê¸°ë°˜)"""
    try:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (classify_with_langchain ë“±)
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        user_areas = []
        if request.user_id:
            try:
                user_context = injector.get_user_context(request.user_id)
                user_areas = user_context.get('areas', [])
                
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                logger.info(f"ğŸ“„ ìƒˆ ë¶„ë¥˜ ìš”ì²­")
                logger.info(f"User ID: {request.user_id}")
                logger.info(f"User Areas: {user_areas}")
                logger.info(f"Text Preview: {request.text[:100]}...")
                logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            except Exception as e:
                logger.warning(f"âš ï¸ Context loading failed: {e}")
        
        # Step 2: AI ë¶„ì„ (LangChain ì‚¬ìš©)
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•¨!
        result = classify_with_langchain(request.text)
        
        # Step 3: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id and user_areas:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
            result["context_injected"] = True
            result["user_areas"] = user_areas
        else:
            result["context_injected"] = False
            result["user_areas"] = []

        # Step 4: ë””ë²„ê¹… ë¡œê·¸
        logger.info(f"âœ… ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"  - Category: {result.get('category', 'N/A')}")
        logger.info(f"  - Tags: {result.get('tags', [])[:5]}")
        logger.info(f"  - Context Injected: {result.get('context_injected', False)}")
        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        return {
            "status": "error",
            "message": str(e)
        }



@router.post("/metadata")
async def classify_metadata_endpoint(request: MetadataClassifyRequest):
    """ë©”íƒ€ë°ì´í„° ë¶„ë¥˜"""
    try:
        result = classify_with_metadata(request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/hybrid")
async def hybrid_classify_endpoint(request: HybridClassifyRequest):
    """í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜"""
    try:
        result = hybrid_classify(request.text, request.metadata)
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/parallel")
async def parallel_classify_endpoint(request: ParallelClassifyRequest):
    """í…ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„° ë³‘ë ¬ ë¶„ë¥˜"""
    try:
        # âš ï¸ ì£¼ì˜: ParallelClassifier.classify_parallelì€ ì •ì  ë©”ì„œë“œ!
        # â†’ ë§¤ë²ˆ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•¨
        result = ParallelClassifier.classify_parallel(
            request.text,
            request.metadata
        )
        
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ ë³‘ë ¬ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


@router.post("/para")
async def classify_para(request: ClassificationRequest):
    """PARA ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
    
        - /api/classify/para ë¡œ ì ‘ê·¼ ê°€ëŠ¥
    """
    try:
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "category": result.get("category", "Resources"),
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/keywords")
async def classify_keywords(request: ClassificationRequest):
    """í‚¤ì›Œë“œ ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸
        - ì ‘ê·¼: POST http://localhost:8000/api/classify/keywords
    """
    try:
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ìš”ì²­: {request.text[:50]}...")
        
        # âš ï¸ ì£¼ì˜: classify_with_langchainì€ ë§¤ë²ˆ ìƒˆë¡œìš´ LLM í˜¸ì¶œ!
        result = classify_with_langchain(request.text)
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        if request.user_id:
            result = injector.inject_context_from_user_id(
                user_id=request.user_id,
                ai_result=result
            )
        
        return {
            "status": "success",
            "category": {
                "keywords": result.get("tags", []),
                "confidence": result.get("confidence", 0.8)
            },
            "result": result
        }
    
    except Exception as e:
        logger.error(f"âŒ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# /classify ì—”ë“œí¬ì¸íŠ¸
# ============================================================
@router.post("/classify", response_model=ClassifyResponse)
async def classify_text(request: ClassifyRequest):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ API
    
    - ë§¤ë²ˆ ìƒˆë¡œìš´ KeywordClassifier ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    - ë¹„ë™ê¸° aclassify() ì‚¬ìš©
    - ì‚¬ìš©ì ë§¥ë½(occupation, areas, interests) ì™„ì „ ë°˜ì˜
    - ìƒˆ keyword_tags ë§¤ë²ˆ ìƒì„±
    - DB ë° ë¡œê·¸ì— ì €ì¥
    """
    try:
        logger.info(f"ğŸ” ë¶„ë¥˜ ìš”ì²­ ì‹œì‘:")
        logger.info(f"   - Text: {request.text[:50]}...")
        logger.info(f"   - User ID: {request.user_id}")
        logger.info(f"   - File ID: {request.file_id}")
        logger.info(f"   - Occupation: {request.occupation}")
        logger.info(f"   - Areas: {request.areas}")
        logger.info(f"   - Interests: {request.interests}")
        
        # ============================================================
        # Step 1: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ============================================================
        
        user_context = {
            "user_id": request.user_id,
            "file_id": request.file_id,
            "occupation": request.occupation or "ì¼ë°˜ ì‚¬ìš©ì",      # ì§ì—…
            "areas": request.areas,                              # ì˜ì—­
            "interests": request.interests,                      # ê´€ì‹¬ì‚¬
            "context_keywords": {                                # ìë™ ìƒì„±
                area: [area, f"{area} ê´€ë ¨", f"{area} ì—…ë¬´", f"{area} í”„ë¡œì íŠ¸"]
                for area in (request.areas or [])
            }
        }
        
        logger.info(f"âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±:")
        logger.info(f"   - Occupation: {user_context['occupation']}")
        logger.info(f"   - Areas: {user_context['areas']}")
        logger.info(f"   - Context Keywords: {list(user_context['context_keywords'].keys())}")

        # ============================================================
        # Step 2: PARA ë¶„ë¥˜ (ë§¤ë²ˆ ìƒˆë¡œ)
        # ============================================================
        try:
            # PARA Agent ì‹¤í–‰
            para_result = await run_para_agent(
                text=request.text,
                metadata={
                    "user_id": request.user_id,
                    "file_id": request.file_id,
                    "occupation": request.occupation,
                    "areas": request.areas,
                    "interests": request.interests          # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
                }
            )
            logger.info(f"âœ… PARA ë¶„ë¥˜ ì™„ë£Œ:")
            logger.info(f"   - Category: {para_result.get('category')}")
            logger.info(f"   - Confidence: {para_result.get('confidence')}")
            logger.info(f"   - Snapshot ID: {para_result.get('snapshot_id')}")
        
        except Exception as para_error:
            logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {para_error}", exc_info=True)
            # ê¸°ë³¸ê°’ ì„¤ì •
            para_result = {
                "category": "Resources",
                "confidence": 0.0,
                "snapshot_id": f"snap_failed_{int(datetime.now().timestamp())}"
            }
        
        # ============================================================
        # Step 3: í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„ë™ê¸° + ìƒˆ ì¸ìŠ¤í„´ìŠ¤)
        # ============================================================
        keyword_classifier = KeywordClassifier()                # ë§¤ë²ˆ ìƒˆ ì¸ìŠ¤í„´ìŠ¤!
        
        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ (Instance ID: {keyword_classifier.instance_id})")
        keyword_result = await keyword_classifier.aclassify(    # ë¹„ë™ê¸° aclassify!
            text=request.text,
            user_context=user_context
        )
        
        # keyword_tags ì¶”ì¶œ ë° ë³´ì¥
        new_keyword_tags = keyword_result.get('tags', ['ê¸°íƒ€'])
        if not isinstance(new_keyword_tags, list):
            new_keyword_tags = [str(new_keyword_tags)] if new_keyword_tags else ['ê¸°íƒ€']
        else:
            new_keyword_tags = [str(tag) for tag in new_keyword_tags if str(tag).strip()]
            if not new_keyword_tags:
                new_keyword_tags = ['ê¸°íƒ€']

        logger.info(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"   - Instance ID: {keyword_result.get('instance_id')}")
        logger.info(f"   - Tags: {new_keyword_tags[:5]}")
        logger.info(f"   - Confidence: {keyword_result.get('confidence')}")
        logger.info(f"   - User Context Matched: {keyword_result.get('user_context_matched')}")
        logger.info(f"   - Processing Time: {keyword_result.get('processing_time')}")

        # ============================================================
        # Step 4: ì¶©ëŒ í•´ê²°
        # ============================================================
        
        conflict_service = ConflictService()
        conflict_result = conflict_service.classify_text(
            para_result=para_result,
            keyword_result=keyword_result,
            text=request.text,
            user_context=user_context                                   # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬
        )

        logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ:")
        logger.info(f"   - Final Category: {conflict_result.get('final_category')}")
        logger.info(f"   - Keyword Tags: {conflict_result.get('keyword_tags', new_keyword_tags)}")
        logger.info(f"   - Conflict Detected: {conflict_result.get('conflict_detected')}")
        logger.info(f"   - Requires Review: {conflict_result.get('requires_review')}")


        # ============================================================
        # Step 5: ì™„ì „ ìˆ˜ì •ëœ DataManager + DB ì €ì¥
        # ============================================================

        # 1. DataManager CSV ë¡œê·¸ ëˆ„ì 
        try:
            data_manager = DataManager()

            csv_log_result = data_manager.log_classification(
                user_id=request.user_id or "anonymous",
                file_name=request.file_id or "unknown",
                ai_prediction=conflict_result.get('final_category') if 'conflict_result' in locals() else 'ê¸°íƒ€',
                user_selected=None,
                confidence=conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0
                # keyword_tags, user_areas ì œê±° - testì™€ ë§¤ê°œë³€ìˆ˜ ì¼ì¹˜ì‹œí‚¤ê¸°
            )
            
            logger.info(f"âœ… CSV ë¡œê·¸ ì €ì¥ ì™„ë£Œ: data/classifications/classification_log.csv")
            csv_saved = True

        except Exception as csv_error:
            logger.warning(f"âš ï¸ CSV ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {csv_error}")
            csv_saved = False


        # 2. ClassificationMetadataExtender DB ì €ì¥
        try:
            from backend.database.metadata_schema import ClassificationMetadataExtender
            
            extender = ClassificationMetadataExtender()
            
            # ì•ˆì „í•œ ë°ì´í„°ë§Œ DBì— ì €ì¥ (Snapshot ê°ì²´ ì œê±°)
            db_result = {
                "category": conflict_result.get('final_category', para_result.get('category', 'Resources')) if 'conflict_result' in locals() else para_result.get('category', 'Resources'),
                "keyword_tags": new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'],
                "confidence": conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0,
                "conflict_detected": conflict_result.get('conflict_detected', False) if 'conflict_result' in locals() else False,
                "requires_review": conflict_result.get('requires_review', False) if 'conflict_result' in locals() else False,
                "snapshot_id": str(para_result.get('snapshot_id', 'snap_unknown')) if 'para_result' in locals() else f"snap_{int(time.time())}",
                "reasoning": conflict_result.get('reason', 'ë¶„ë¥˜ ì™„ë£Œ') if 'conflict_result' in locals() else 'ë¶„ë¥˜ ì™„ë£Œ',
                "user_context": {
                    "user_id": request.user_id or "anonymous",
                    "occupation": request.occupation,
                    "areas": request.areas,
                    "interests": request.interests
                }
            }
            
            db_filename = f"{request.user_id or 'anonymous'}_{int(time.time())}"
            
            saved_file_id = extender.save_classification_result(
                result=db_result,
                filename=db_filename
            )
            
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: file_id={saved_file_id}")

        except Exception as db_error:
            logger.warning(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {db_error}")
            saved_file_id = None


        # 3. ê°„ë‹¨í•œ JSON ë¡œê·¸ (ëª¨ë“  ê°ì²´ ì•ˆì „ ì²˜ë¦¬)
        try:
            from pathlib import Path
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # ì•ˆì „í•œ JSON ë°ì´í„°ë§Œ ì‚¬ìš©
            safe_snapshot_id = str(para_result.get('snapshot_id', 'snap_unknown')) if 'para_result' in locals() else 'snap_unknown'
            
            simple_log = {
                "timestamp": timestamp,
                "user_id": request.user_id or "anonymous",
                "text_preview": request.text[:100],
                "category": conflict_result.get('final_category', 'Resources') if 'conflict_result' in locals() else 'Resources',
                "confidence": float(conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0),
                "keyword_tags": new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'],
                "snapshot_id": safe_snapshot_id,
                "user_areas": request.areas,
                "matched_context": keyword_result.get('user_context_matched', False) if 'keyword_result' in locals() else False
            }
            
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            LOG_DIR.mkdir(parents=True, exist_ok=True)
            
            json_filename = LOG_DIR / f"classification_clean_{timestamp}.json"
            
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(simple_log, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… JSON ë¡œê·¸ ì €ì¥: {json_filename.name}")
            json_saved = True

        except Exception as json_error:
            logger.warning(f"âš ï¸ JSON ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {json_error}")
            json_saved = False
            

        # ========== ì €ì¥ ë¡œì§ ì¶”ê°€: classification_log.csv ì§ì ‘ ì €ì¥ (ë°±ì—…) ==========
        try:
            from pathlib import Path
            import csv
            
            # CSV íŒŒì¼ ê²½ë¡œ (flownote-mvp/data/classifications/)
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            CSV_DIR = PROJECT_ROOT / "data" / "classifications"
            CSV_DIR.mkdir(parents=True, exist_ok=True)
            CSV_PATH = CSV_DIR / "classification_log.csv"
            
            # CSV í—¤ë” í™•ì¸ í›„ ì¶”ê°€
            file_exists = CSV_PATH.exists()
            
            with open(CSV_PATH, mode='a', newline='', encoding='utf-8') as f:
                fieldnames = ['timestamp', 'user_id', 'file_id', 'category', 'confidence', 'keyword_tags']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                if not file_exists:
                    writer.writeheader()
                
                writer.writerow({
                    'timestamp': datetime.now().isoformat(),
                    'user_id': request.user_id or "anonymous",
                    'file_id': request.file_id or "unknown",
                    'category': conflict_result.get('final_category', 'Resources') if 'conflict_result' in locals() else 'Resources',
                    'confidence': round(conflict_result.get('confidence', 0.0), 2) if 'conflict_result' in locals() else 0.0,
                    'keyword_tags': ','.join(new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'])
                })
            
            logger.info(f"âœ… CSV ì§ì ‘ ì €ì¥ ì™„ë£Œ: {CSV_PATH}")
            csv_direct_saved = True

        except Exception as csv_error:
            logger.warning(f"âš ï¸ CSV ì§ì ‘ ì €ì¥ ì‹¤íŒ¨: {csv_error}")
            csv_direct_saved = False
        # ========== CSV ì§ì ‘ ì €ì¥ ë ==========


        # ========== ì €ì¥ ë¡œì§ ì¶”ê°€ 2: data/log/ JSON íŒŒì¼ ì €ì¥ ==========
        try:
            from pathlib import Path
            import json
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            
            # JSON ë¡œê·¸ ê²½ë¡œ (flownote-mvp/data/log/)
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            LOG_DIR.mkdir(parents=True, exist_ok=True)
            
            safe_snapshot_id = str(para_result.get('snapshot_id', 'snap_unknown')) if 'para_result' in locals() else 'snap_unknown'
            
            log_data = {
                "timestamp": timestamp,
                "user_id": request.user_id or "anonymous",
                "file_id": request.file_id or "unknown",
                "text_preview": request.text[:100],
                "category": conflict_result.get('final_category', 'Resources') if 'conflict_result' in locals() else 'Resources',
                "confidence": float(conflict_result.get('confidence', 0.0) if 'conflict_result' in locals() else 0.0),
                "keyword_tags": new_keyword_tags if 'new_keyword_tags' in locals() else ['ê¸°íƒ€'],
                "snapshot_id": safe_snapshot_id,
                "user_areas": request.areas,
                "matched_context": keyword_result.get('user_context_matched', False) if 'keyword_result' in locals() else False
            }
            
            json_filename = LOG_DIR / f"classification_{timestamp}.json"
            
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            
            logger.info(f"âœ… JSON ë¡œê·¸ ì €ì¥: {json_filename.name}")
            json_saved = True

        except Exception as json_error:
            logger.warning(f"âš ï¸ JSON ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {json_error}")
            json_saved = False
        # ========== JSON ì €ì¥ ë ==========


        # ========== ì €ì§• ë¡œì§ ì¶”ê°€_3 : user_context_mapping.json ëˆ„ì  ì €ì¥ ==========

        try:
            from pathlib import Path
            import json
            from datetime import datetime  # âœ… ë°˜ë“œì‹œ í•„ìš”
            
            # user_context_mapping.json ê²½ë¡œ (flownote-mvp/data/context/)
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            CONTEXT_DIR = PROJECT_ROOT / "data" / "context"
            CONTEXT_DIR.mkdir(parents=True, exist_ok=True)
            CONTEXT_PATH = CONTEXT_DIR / "user_context_mapping.json"
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            if CONTEXT_PATH.exists():
                with open(CONTEXT_PATH, "r", encoding="utf-8") as f:
                    context_data = json.load(f)
            else:
                context_data = {}
            
            # ìˆ˜ì •: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            try:
                # âœ… FastAPI requestì—ì„œ user_id ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ anonymous)
                user_id = getattr(request, "user_id", None) or "anonymous"
                
                # âœ… ê¸°ë³¸ êµ¬ì¡° ë¨¼ì € ë³´ì¥
                if user_id not in context_data:
                    context_data[user_id] = {
                        "occupation": getattr(request, "occupation", None) or "ì¼ë°˜ ì‚¬ìš©ì",
                        "areas": getattr(request, "areas", None) or [],
                        "interests": getattr(request, "interests", None) or [],
                        "recent_categories": [],
                        "total_classifications": 0,
                        "last_updated": datetime.now().isoformat()
                    }
                
                # âœ… ê° í•„ë“œ ì•ˆì „í•˜ê²Œ ì—…ë°ì´íŠ¸
                if "recent_categories" not in context_data[user_id]:
                    context_data[user_id]["recent_categories"] = []
                if "total_classifications" not in context_data[user_id]:
                    context_data[user_id]["total_classifications"] = 0
                
                # ìµœê·¼ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                final_category = conflict_result.get('final_category', 'Resources') if conflict_result else 'Resources'
                context_data[user_id]["recent_categories"].append(final_category)
                context_data[user_id]["recent_categories"] = context_data[user_id]["recent_categories"][-10:]
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                context_data[user_id]["total_classifications"] += 1
                context_data[user_id]["last_updated"] = datetime.now().isoformat()
                
                # ì €ì¥
                with open(CONTEXT_PATH, "w", encoding="utf-8") as f:
                    json.dump(context_data, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… user_context_mapping.json ì €ì¥: {user_id}")
                context_saved = True
            except Exception as context_error:
                logger.warning(f"âš ï¸ user_context_mapping.json ì €ì¥ ì‹¤íŒ¨: {context_error}")
                context_saved = False

        except Exception as context_error:
            logger.warning(f"âš ï¸ user_context_mapping.json ì €ì¥ ì‹¤íŒ¨: {context_error}")
            context_saved = False

        # ========== user_context_mapping.json ì €ì¥ ë ==========


        # 4. log_info ìƒì„±
        log_info = {
            "csv_log": "data/classifications/classification_log.csv",
            "db_saved": saved_file_id is not None,
            "csv_direct_saved": csv_direct_saved if 'csv_direct_saved' in locals() else False,
            "json_log": json_filename.name if 'json_filename' in locals() and json_saved else None,
            "context_saved": context_saved if 'context_saved' in locals() else False,
            "log_directory": "data/log"
        }

        logger.info(f"âœ… Step 5 ì™„ë£Œ - CSV DataManager: {csv_saved}, CSV Direct: {csv_direct_saved if 'csv_direct_saved' in locals() else False}, JSON: {json_saved if 'json_saved' in locals() else False}, Context: {context_saved if 'context_saved' in locals() else False}")


        # ============================================================
        # Step 6: ì‘ë‹µ ë°˜í™˜
        # ============================================================
        
        # ìˆ˜ì • (ìš°ì„ ìˆœìœ„ ì¡°ì •)
        final_category = conflict_result.get('final_category', para_result.get('category', 'ê¸°íƒ€'))
        category = final_category if final_category != 'None' else para_result.get('category', 'Resources')
        
        response = ClassifyResponse(
            category=category,
            confidence=conflict_result.get('confidence', para_result.get('confidence', 0.0)),
            snapshot_id=str(para_result.get('snapshot_id', '')),
            conflict_detected=conflict_result.get('conflict_detected', False),
            requires_review=conflict_result.get('requires_review', False),
            keyword_tags=new_keyword_tags,                      # ìƒˆë¡œ ìƒì„±ëœ í‚¤ì›Œë“œ
            reasoning=conflict_result.get('reason', ''),
            
            # ì‚¬ìš©ì ë§¥ë½ ê´€ë ¨ (ìƒˆ í•„ë“œë“¤)
            user_context_matched=keyword_result.get('user_context_matched', False),
            user_areas=request.areas,                           # ìš”ì²­ëœ ì˜ì—­
            user_context=user_context,                          # ì „ë‹¬ëœ ì „ì²´ ì»¨í…ìŠ¤íŠ¸
            context_injected=len(request.areas) > 0,            # ë§¥ë½ ì£¼ì… ì—¬ë¶€
            log_info=log_info,                                  # ë¡œê·¸ ì •ë³´
            csv_log_result=csv_log_result if 'csv_log_result' in locals() else {},                      # CSV ë¡œê·¸ ê²°ê³¼
        )

        logger.info(f"âœ… ì „ì²´ ë¶„ë¥˜ ì™„ë£Œ!")
        logger.info(f"   - Final Category: {response.category}")
        logger.info(f"   - Keyword Tags: {response.keyword_tags[:3]}...")
        logger.info(f"   - User Context Matched: {response.user_context_matched}")
        logger.info(f"   - Total Time: ~{keyword_result.get('processing_time', 'N/A')}")

        return response

    except Exception as e:
        logger.error(f"âŒ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")


# ============================================================
# íŒŒì¼ ì—…ë¡œë“œ ë¶„ë¥˜ ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • (ë¡œê·¸ ì¶”ê°€)
# ============================================================
"""
backend/routes/classifier_routes.py
íŒŒì¼ ì—…ë¡œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì „ìš© ë¼ìš°í„°
"""

@router.post("/file", response_model=ClassifyResponse)
async def classify_file(
    request: Request, 
    file: UploadFile = File(...),
    user_id: Optional[str] = Form(None),
    file_id: Optional[str] = Form(None),
    occupation: Optional[str] = Form(None),
    areas: Optional[str] = Form(None),
    interests: Optional[str] = Form(None),
    selected_category: Optional[str] = Form(None)
):
    """íŒŒì¼ ì—…ë¡œë“œ ê¸°ë°˜ ë¶„ë¥˜ API - Form ë°ì´í„° + ì¤‘ë³µ ì €ì¥ ë¡œì§ í¬í•¨"""
    try:
        import json
        from pathlib import Path
        import csv
        import time

        # === ì´ˆê¸° ìƒíƒœê°’ ë³´ì¥ ===
        csv_saved = False
        csv_direct_saved = False
        json_saved = False
        context_saved = False
        csv_log_result = None
        json_filename = None
        areas_list = None
        
        # ============================================================
        # Step 1: íŒŒì¼ ì½ê¸° + Form ë°ì´í„° íŒŒì‹±
        # ============================================================
        content = await file.read()
        text = content.decode("utf-8", errors="ignore") if isinstance(content, (bytes, bytearray)) else str(content)

        # JSON stringì„ listë¡œ ë³€í™˜ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        try:
            areas_list = json.loads(areas) if areas else []
        except Exception:
            areas_list = []
        try:
            interests_list = json.loads(interests) if interests else []
        except Exception:
            interests_list = []

        logger.info(f"ğŸ“‚ íŒŒì¼ ì½ê¸°: {file.filename}")

        # ============================================================
        # Step 2: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ============================================================
        # ìš°ì„  Formìœ¼ë¡œ ì „ë‹¬ëœ user_idë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ request.stateì—ì„œ ì°¾ìŒ
        effective_user_id = user_id or getattr(request.state, "user_id", None) or "anonymous"
        
        # âœ… DataManagerì—ì„œ ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ
        data_manager = DataManager()
        user_profile = data_manager.get_user_profile(effective_user_id)
        
        # í”„ë¡œí•„ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ Form ë°ì´í„° ì‚¬ìš©
        if user_profile:
            occupation_value = user_profile.get("occupation", "ì¼ë°˜ ì‚¬ìš©ì")
            # CSVì—ì„œ ì½ì€ areasëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            stored_areas = user_profile.get("areas", "")
            areas_list = [a.strip() for a in stored_areas.split(",")] if stored_areas else areas_list or []
            stored_interests = user_profile.get("interests", "")
            interests_list = [i.strip() for i in stored_interests.split(",")] if stored_interests else interests_list or []
            logger.info(f"âœ… í”„ë¡œí•„ ë¡œë“œ ì„±ê³µ: {occupation_value}, areas={len(areas_list)}ê°œ")
        else:
            occupation_value = occupation or "ì¼ë°˜ ì‚¬ìš©ì"
            logger.warning(f"âš ï¸ í”„ë¡œí•„ ì—†ìŒ, Form ë°ì´í„° ì‚¬ìš©")

        user_context = {
            "user_id": effective_user_id,
            "file_id": file_id or file.filename,
            "occupation": occupation_value,
            "areas": areas_list,
            "interests": interests_list,
            "context_keywords": {
                area: [area, f"{area} ê´€ë ¨", f"{area} ì—…ë¬´", f"{area} í”„ë¡œì íŠ¸"]
                for area in areas_list
            } if areas_list else {}
        }

        logger.info(f"ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...:")
        logger.info(f"   - Occupation: {user_context['occupation']}")
        logger.info(f"   - Areas: {user_context['areas']}")
        logger.info(f"   - Context Keywords: {list(user_context['context_keywords'].keys())}")
        
        logger.info(f"ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (user_id={effective_user_id})")

        # ============================================================
        # Step 3: PARA ë¶„ë¥˜
        # ============================================================
        try:
            para_result = await run_para_agent(
                text=text,
                metadata={
                    "user_id": effective_user_id,
                    "file_id": file_id or file.filename,
                    "occupation": occupation,
                    "areas": areas_list,
                    "interests": interests_list
                    #"user_id": request.user_id,
                    #"file_id": request.file_id,
                    #"occupation": request.occupation,
                    #"areas": request.areas,
                    #"interests": request.interests          # ì‚¬ìš©ì ë§¥ë½ ì „ë‹¬            
                }
            )
            logger.info(f"âœ… PARA ë¶„ë¥˜ ì™„ë£Œ:")
            logger.info(f"   - Category: {para_result.get('category')}")
            logger.info(f"   - Confidence: {para_result.get('confidence')}")
            logger.info(f"   - Snapshot ID: {para_result.get('snapshot_id')}")
            
        except Exception as para_error:
            logger.error(f"âŒ PARA ë¶„ë¥˜ ì‹¤íŒ¨: {para_error}", exc_info=True)
            para_result = {
                "category": "Resources",
                "confidence": 0.0,
                "snapshot_id": f"snap_failed_{int(datetime.now(timezone.utc).timestamp())}"
            }

        # ============================================================
        # Step 4: í‚¤ì›Œë“œ ì¶”ì¶œ
        # ============================================================

        keyword_classifier = KeywordClassifier()                # ë§¤ë²ˆ ìƒˆ ì¸ìŠ¤í„´ìŠ¤!

        logger.info(f"ğŸ” í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ (Instance ID: {keyword_classifier.instance_id})")

        # âœ… ìˆ˜ì •: aclassify í˜¸ì¶œ í›„ ì•ˆì „í•˜ê²Œ tags ì¶”ì¶œ
        keyword_result = await keyword_classifier.aclassify(
            text=text,
            user_context=user_context
        )

        # âœ… í•µì‹¬ ìˆ˜ì •: tags ì¶”ì¶œ ë¡œì§ ê°•í™”
        raw_tags = keyword_result.get('tags', [])
        logger.info(f"ğŸ“¦ Raw tags from LLM: {raw_tags} (type: {type(raw_tags)})")

        # 1. Noneì´ê±°ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if not raw_tags:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  Tagsê°€ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {new_keyword_tags}")
            
        # 2. ë¬¸ìì—´ì¸ ê²½ìš° (LLMì´ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë¬¸ìì—´ë¡œ ë°˜í™˜í•œ ê²½ìš°)
        elif isinstance(raw_tags, str):
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì¸ì§€ í™•ì¸
            if ',' in raw_tags:
                new_keyword_tags = [tag.strip() for tag in raw_tags.split(',') if tag.strip()]
            else:
                new_keyword_tags = [raw_tags.strip()] if raw_tags.strip() else ['ê¸°íƒ€']
            logger.info(f"âœ… ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜: {new_keyword_tags}")

        # 3. ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì •ìƒ)
        elif isinstance(raw_tags, list):
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ëª¨ë“  ìš”ì†Œê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
            valid_tags = [str(tag).strip() for tag in raw_tags if tag and str(tag).strip()]
            new_keyword_tags = valid_tags if valid_tags else ['ê¸°íƒ€']
            logger.info(f"âœ… ë¦¬ìŠ¤íŠ¸ ê²€ì¦ ì™„ë£Œ: {len(valid_tags)}ê°œ íƒœê·¸")

        # 4. ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…
        else:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… {type(raw_tags)}, ê¸°ë³¸ê°’ ì‚¬ìš©")

        # âœ… ìµœì¢… ê²€ì¦: ìµœì†Œ 1ê°œ ì´ìƒì˜ íƒœê·¸ ë³´ì¥
        if not new_keyword_tags or len(new_keyword_tags) == 0:
            new_keyword_tags = ['ê¸°íƒ€']
            logger.warning(f"âš ï¸  ìµœì¢… ê²€ì¦ ì‹¤íŒ¨, ê°•ì œ ê¸°ë³¸ê°’ ì„¤ì •")

        logger.info(f"âœ… í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ:")
        logger.info(f"   - Instance ID: {keyword_result.get('instance_id')}")
        logger.info(f"   - Final Tags: {new_keyword_tags[:5]}")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
        logger.info(f"   - Tags Count: {len(new_keyword_tags)}")
        logger.info(f"   - Confidence: {keyword_result.get('confidence')}")
        logger.info(f"   - User Context Matched: {keyword_result.get('user_context_matched')}")
        logger.info(f"   - Processing Time: {keyword_result.get('processing_time')}")

        # ============================================================
        # Step 5: ì¶©ëŒ í•´ê²°
        # ============================================================
        conflict_service = ConflictService()
        conflict_result = conflict_service.classify_text(
            para_result=para_result,
            keyword_result=keyword_result,
            text=text,
            user_context=user_context
        )

        logger.info(f"âœ… ì¶©ëŒ í•´ê²° ì™„ë£Œ: {conflict_result.get('final_category')}")

        # ============================================================
        # Step 6-1: DataManager CSV ë¡œê·¸ ì €ì¥
        # ============================================================
        try:
            data_manager = DataManager()
            csv_log_result = data_manager.log_classification(
                user_id=effective_user_id,
                file_name=file_id or file.filename or "unknown",
                ai_prediction=conflict_result.get('final_category', 'Resources'),
                user_selected=selected_category,
                confidence=conflict_result.get('confidence', 0.0)
            )
            logger.info(f"âœ… CSV DataManager ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
            csv_saved = True
        except Exception as csv_error:
            logger.warning(f"âš ï¸ CSV DataManager ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {csv_error}", exc_info=True)
            csv_saved = False
            csv_log_result = None

        # ============================================================
        # Step 6-2: CSV ì§ì ‘ ì €ì¥ (ë°±ì—…)
        # ============================================================
        try:
            PROJECT_ROOT = Path(__file__).parent.parent.parent
            CSV_DIR = PROJECT_ROOT / "data" / "classifications"
            CSV_DIR.mkdir(parents=True, exist_ok=True)
            CSV_PATH = CSV_DIR / "classification_log.csv"

            file_exists = CSV_PATH.exists()

            with open(CSV_PATH, mode='a', newline='', encoding='utf-8') as f:
                fieldnames = ['timestamp', 'user_id', 'file_id', 'category', 'confidence', 'keyword_tags']
                writer = csv.DictWriter(f, fieldnames=fieldnames)

                if not file_exists:
                    writer.writeheader()

                writer.writerow({
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'user_id': effective_user_id,
                    'file_id': file_id or file.filename,
                    'category': conflict_result.get('final_category', 'Resources'),
                    'confidence': round(conflict_result.get('confidence', 0.0), 2),
                    'keyword_tags': ','.join(new_keyword_tags)
                })

            logger.info(f"âœ… CSV ì§ì ‘ ì €ì¥ ì™„ë£Œ: {CSV_PATH}")
            csv_direct_saved = True
        except Exception as csv_error:
            logger.warning(f"âš ï¸ CSV ì§ì ‘ ì €ì¥ ì‹¤íŒ¨: {csv_error}", exc_info=True)
            csv_direct_saved = False

        # ============================================================
        # Step 6-3: JSON ë¡œê·¸ ì €ì¥
        # ============================================================
        try:
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S_%f")[:-3]
            LOG_DIR = PROJECT_ROOT / "data" / "log"
            LOG_DIR.mkdir(parents=True, exist_ok=True)

            log_data = {
                "timestamp": timestamp,
                "user_id": effective_user_id,
                "file_id": file_id or file.filename,
                "text_preview": text[:100],
                "category": conflict_result.get('final_category', 'Resources'),
                "confidence": float(conflict_result.get('confidence', 0.0)),
                "keyword_tags": new_keyword_tags,
                "snapshot_id": str(para_result.get('snapshot_id', 'snap_unknown')),
                "user_areas": areas_list,
                "matched_context": keyword_result.get('user_context_matched', False)
            }

            json_filename = LOG_DIR / f"classification_{timestamp}.json"
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… JSON ë¡œê·¸ ì €ì¥: {json_filename.name}")
            json_saved = True
        except Exception as json_error:
            logger.warning(f"âš ï¸ JSON ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {json_error}", exc_info=True)
            json_saved = False
            json_filename = None

        # ============================================================
        # Step 6-4: user_context_mapping.json ì €ì¥ (ì•ˆì „í•˜ê²Œ)
        # ============================================================
        try:
            # user_context_mapping.json ê²½ë¡œ (flownote-mvp/data/context/)
            CONTEXT_DIR = PROJECT_ROOT / "data" / "context"
            CONTEXT_DIR.mkdir(parents=True, exist_ok=True)
            CONTEXT_PATH = CONTEXT_DIR / "user_context_mapping.json"

            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            if CONTEXT_PATH.exists():
                with open(CONTEXT_PATH, "r", encoding="utf-8") as f:
                    context_data = json.load(f)
            else:
                context_data = {}

            # ì‚¬ìš©ì ID (ì´ë¯¸ effective_user_idì— ìˆìŒ)
            uid = effective_user_id
            final_category = conflict_result.get('final_category', 'Resources') if conflict_result else 'Resources'

            # ê¸°ë³¸ êµ¬ì¡° ë³´ì¥
            context_data.setdefault(uid, {
                "occupation": occupation or "ì¼ë°˜ ì‚¬ìš©ì",
                "areas": areas_list or [],
                "interests": interests_list or [],
                "recent_categories": [],
                "total_classifications": 0,
                "last_updated": datetime.now(timezone.utc).isoformat()
            })

            # ì•ˆì „ ì—…ë°ì´íŠ¸
            user_ctx = context_data[uid]
            user_ctx.setdefault("recent_categories", [])
            user_ctx.setdefault("total_classifications", 0)

            user_ctx["recent_categories"].append(final_category)
            # ì¤‘ë³µ ì œê±° + ìµœê·¼ 10ê°œ ìœ ì§€
            user_ctx["recent_categories"] = list(dict.fromkeys(user_ctx["recent_categories"][-10:]))

            user_ctx["total_classifications"] += 1
            user_ctx["last_updated"] = datetime.now(timezone.utc).isoformat()

            # ì•ˆì „í•˜ê²Œ ì„ì‹œíŒŒì¼ì— ì“°ê³  êµì²´
            temp_path = CONTEXT_PATH.with_suffix(".tmp")
            with open(temp_path, "w", encoding="utf-8") as f:
                json.dump(context_data, f, ensure_ascii=False, indent=2)
            temp_path.replace(CONTEXT_PATH)

            logger.info(f"âœ… user_context_mapping.json ì €ì¥: {uid}")
            context_saved = True
        except Exception as context_error:
            logger.warning(f"âš ï¸ user_context_mapping.json ì €ì¥ ì‹¤íŒ¨: {context_error}", exc_info=True)
            context_saved = False

        # ============================================================
        # Step 6-5: log_info ìƒì„±
        # ============================================================
        log_info = {
            "csv_log": str(CSV_PATH) if 'CSV_PATH' in locals() else "data/classifications/classification_log.csv",
            "db_saved": False,
            "csv_direct_saved": csv_direct_saved,
            "json_log": json_filename.name if json_filename and json_saved else None,
            "context_saved": context_saved,
            "log_directory": str(LOG_DIR) if 'LOG_DIR' in locals() else "data/log"
        }

        logger.info(
            f"âœ… ì „ì²´ ë¡œê·¸ ì €ì¥ ì™„ë£Œ - CSV DataManager: {csv_saved}, CSV Direct: {csv_direct_saved}, JSON: {json_saved}, Context: {context_saved}"
        )

        # ============================================================
        # Step 7: ì‘ë‹µ ë°˜í™˜
        # ============================================================
        final_category = conflict_result.get('final_category', para_result.get('category', 'Resources'))

        response = ClassifyResponse(
            category=final_category,
            confidence=conflict_result.get('confidence', para_result.get('confidence', 0.0)),
            snapshot_id=str(para_result.get('snapshot_id', '')),
            conflict_detected=conflict_result.get('conflict_detected', False),
            requires_review=conflict_result.get('requires_review', False),
            keyword_tags=new_keyword_tags,
            reasoning=conflict_result.get('reason', ''),
            user_context_matched=keyword_result.get('user_context_matched', False),
            user_areas=areas_list,
            user_context=user_context,
            context_injected=len(areas_list) > 0,
            log_info=log_info,
            csv_log_result=csv_log_result
        )

        logger.info(f"âœ… ì „ì²´ íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ!")
        return response

    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")




